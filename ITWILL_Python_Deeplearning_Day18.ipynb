{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ Object detection (객체 검출) 실습\n",
    "\n",
    "### ★ 1. 아래의 github에서 실습파일을 다운로드 받습니다.\n",
    "\n",
    "https://github.com/heartkilla/yolo-v3#notification-settings\n",
    "\n",
    "현재 : d드라이브 data4 폴더에 들어가있는 상태\n",
    "\n",
    "### ※ 설명 : data 폴더에 images 안에 사진 2장을 확인하세요. 나중에 점심시간에 거리의 사진을 스마트폰에 담아오세요~\n",
    "\n",
    "1. load_weights.py : 내려받은 가중치를 신경망에 로드하는 코드\n",
    "\n",
    "아래의 사이트에서 가중치를 내려받고 weights폴더에 넣으시오~\n",
    "\n",
    "https://pjreddie.com/media/files/yolov3.weights\n",
    "\n",
    "2. detect.py : 신경망 모델이 들어있는 코드이고 사진속에 이미지를 찾아내서 bounding 하는 실행 코드.\n",
    "\n",
    "목표 : 우리가 object detection 할 이미지의 신경망의 가중치를 만들어 내는것을 목표로 하고 위의 두 스크립트를 활용한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ★ 2. 아나콘다 가상환경을 만들고 pip 명령어로 requirements.txt를 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 파이썬 버젼을 확인합니다.\n",
    "\n",
    "\n",
    "\n",
    "python --version\n",
    "\n",
    "\n",
    "\n",
    "2. 텐써플로우 1.4 는 파이썬 3.5 버젼에서 가능하므로 파이썬 3.5 버젼에 해당하는 가상 환경을 만듭니다.\n",
    "\n",
    "\n",
    "\n",
    "conda create -n snowdeer_yolo python=3.5\n",
    "\n",
    "\n",
    "\n",
    "3. 가상환경을 활성화 시킵니다.\n",
    "\n",
    "\n",
    "\n",
    "activate snowdeer_env\n",
    "\n",
    "\n",
    "4. 디렉토리를 이동한다.\n",
    "(snowdeer_yolo) C:\\Users\\stu>cd D:\\data4\\yolo-v3-master\\yolo-v3-master\n",
    "\n",
    "(snowdeer_yolo) C:\\Users\\stu>d:\n",
    "\n",
    "(snowdeer_yolo) D:\\data4\\yolo-v3-master\\yolo-v3-master>\n",
    "\n",
    "[cd -> 디렉토리를 이동하는 명령어]\n",
    "안되면 한 경로씩 노가다로 cd로 설정해도됨.\n",
    "\n",
    "\n",
    "5. pip 명령어를 아래와 같이 수행한다.\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "Tip. 가상환경 지우기\n",
    "\n",
    "conda remove --name snowdeer_env --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ★ 3. 저자가 만든 가중치를 신경망에 로드한다.\n",
    "\n",
    "python load_weights.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ★ 4. 모델을 실행한다.\n",
    "\n",
    "python detect.py images 0.5 0.5 __data/images/dog.jpg__ __data/images/office.jpg__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ★ 5. detections 폴더에 결과 사진을 확인하세요!\n",
    "\n",
    "D:\\data4\\yolo-v3-master\\detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 점심시간 문제. 사진을 아무거나 골라서 object detection을 구현하고 10기 카톡방에 올리고 검사 받으시오.\n",
    "\n",
    "점심시간에 거리의 사진과 동영상을 찍어옵니다.  \n",
    "알고리즘 조장 모임은 5시  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 동영상을 object detection 하는 방법\n",
    "\n",
    "1. D:\\data4\\yolo-v3-master\\data\\video 폴더로 가서 shinjuku.mp4를 실행하세요~\n",
    "\n",
    "2. 먼저 object detection 된 결과 영상을 확인한다.\n",
    "\n",
    "3. 점심시간 시간 촬영한 동영상을 video 폴더에 넣으시오!\n",
    "\n",
    "4. 코덱을 아래의 사이트에서 다운로드 받습니다.\n",
    "\n",
    "openh264-1.8.0-win64.dll\n",
    "\n",
    "detect.py 에 있는 곳에 다운로드 받으세요~\n",
    "\n",
    "혹시 모르니, video 폴더와 weight 폴더에도 넣으세요~\n",
    "\n",
    "\n",
    "5. detect.py 를 실행한다.\n",
    "\n",
    "python detect.py video 0.5 0.5 data/video/soccer.mp4\n",
    "\n",
    "6. 반 카톡에 영상을 올린다.\n",
    "\n",
    "저자가 만든 신경망에는 유해진/박지성 사진은 없으므로\n",
    "유해진/박지성 사진을 분류하거나 바닷속의 해파리/비닐을 분류하고자하면 해당 사진을 학습한 신경망에 가중치가 있어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ keras로 신경망을 구성하고 학습시키는 방법을 배워보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * keras란 ? 신경망을 만들기 위한 고수준 파이썬 라이브러리\n",
    "\n",
    "keras에 대한 전반적인 설명 : https://cyc1am3n.github.io/2018/11/02/introduction-to-keras.html\n",
    "\n",
    "- 동일한 코드로 CPU와 GPU에서 실행 할 수 있습니다.\n",
    "\n",
    "- 사용하기 쉬운 API를 가지고 있어 딥러닝 모델의 프로토타입을 빠르게 만들 수 있습니다.\n",
    "\n",
    "- (컴퓨터 비전을 위한) CNN, (시퀀스 처리를 위한) RNN을 지원하며 이 둘을 자유롭게 조합하여 사용할 수 있습니다.\n",
    "\n",
    "- 다중 입력이나 다중 출력 모델, 층의 공유, 모델 공유 등 어떤 네트워크 구조도 만들 수 있습니다. 이 말은 GAN(Generative Adversarial Network) \n",
    "부터 뉴럴 튜링 머신까지 케라스는 기본적으로 어떤 딥러닝 모델에도 적합하다는 뜻입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ mnist 필기체 데이터를 keras를 이용해서 분류하는 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "from keras.datasets import mnist # mnist 뿐만 아니라 cifar10 이미지도 있음.\n",
    "from keras import models \n",
    "from keras import layers # 층을 쌓기 위한 기본 모듈을 제공\n",
    "from keras.utils import to_categorical # one hot encoding을 쉽게하는 모듈\n",
    "\n",
    "# MNIST 데이터셋 불러오기 ( 훈련 : 60000장 , 테스트 : 10000장)\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 이미지 데이터 준비하기 (모델에 맞는 크기로 바꾸고 0과 1사이로 스케일링)\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255 \n",
    "# 0에서 1사이로 스케일링을 하기 위해 /255\n",
    "# 나누기 255를 함으로써 표준정규분포에 가깝게 하여 학습이 더 잘되게 해준다,\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# 레이블을 범주형으로 인코딩\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# 모델 정의하기 (여기에서는 Sequential 클래스 사용) // 층쌓는것을 설계하는 부분\n",
    "model = models.Sequential() # 완전 연결 계층 모델\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,))) # 1층 : 512개의 노드\n",
    "model.add(layers.Dense(10, activation='softmax')) # 출력층 : 10개의 노드\n",
    "\n",
    "# 모델 컴파일 하기( optimizer 부분)\n",
    "model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy', # 교차 엔트로피\n",
    "                metrics=['accuracy']) # 정확도를 기준으로 compile을 하겠다.\n",
    "\n",
    "# fit() 메서드로 모델 훈련 시키기\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128, shuffle = True)\n",
    "# 설명 : shuffle = True이면 훈련 스텝마다 배치를 훈련 데이터에서 중복을 허용하지 않고 랜덤하게 추출합니다.\n",
    "\n",
    "# 테스트 데이터로 정확도 측정하기(모델을 평가하는 부분 // 10000장을 한꺼번에 집어넣음.)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test_acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제118. 현재의 2층 신경망을 3층 신경망으로 변경하시오.\n",
    "\n",
    "현재 : 입력층 -----> 은닉1층(512개) -----> 출력층(10개)\n",
    "\n",
    "변경 : 입력층 -----> 은닉1층(100개) -----> 은닉2층(50개) -----> 출력층(10개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "from keras.datasets import mnist # mnist 뿐만 아니라 cifar10 이미지도 있음.\n",
    "from keras import models \n",
    "from keras import layers # 층을 쌓기 위한 기본 모듈을 제공\n",
    "from keras.utils import to_categorical # one hot encoding을 쉽게하는 모듈\n",
    "\n",
    "# MNIST 데이터셋 불러오기 ( 훈련 : 60000장 , 테스트 : 10000장)\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 이미지 데이터 준비하기 (모델에 맞는 크기로 바꾸고 0과 1사이로 스케일링)\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255 # 0에서 1사이로 스케일링을 하기 위해 /255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# 레이블을 범주형으로 인코딩\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# 모델 정의하기 (여기에서는 Sequential 클래스 사용) // 층쌓는것을 설계하는 부분\n",
    "model = models.Sequential() # 완전 연결 계층 모델\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(28 * 28,))) # 1층 : 100개의 노드\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(100,))) # 2층 : 50개의 노드\n",
    "model.add(layers.Dense(10, activation='softmax')) # 출력층 : 10개의 노드\n",
    "\n",
    "# 모델 컴파일 하기( optimizer 부분)\n",
    "model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy', # 교차 엔트로피\n",
    "                metrics=['accuracy']) # 정확도를 기준으로 compile을 하겠다.\n",
    "\n",
    "# fit() 메서드로 모델 훈련 시키기\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128, shuffle = True) # ,verbose =0 을 쓰면 진행과정이 안나타남.\n",
    "# 설명 : shuffle = True이면 훈련 스텝마다 배치를 훈련 데이터에서 중복을 허용하지 않고 랜덤하게 추출합니다.\n",
    "\n",
    "# 테스트 데이터로 정확도 측정하기(모델을 평가하는 부분 // 10000장을 한꺼번에 집어넣음.)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test_acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 이미지를 keras로 훈련시키는 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제1. 위의 기존 mnist 코드를 가져와서 아래와 같이 수정하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "from keras.datasets import cifar10 # mnist 뿐만 아니라 cifar10 이미지도 있음.\n",
    "from keras import models \n",
    "from keras import layers # 층을 쌓기 위한 기본 모듈을 제공\n",
    "from keras.utils import to_categorical # one hot encoding을 쉽게하는 모듈\n",
    "\n",
    "# MNIST 데이터셋 불러오기 ( 훈련 : 60000장 , 테스트 : 10000장)\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# 이미지 데이터 준비하기 (모델에 맞는 크기로 바꾸고 0과 1사이로 스케일링)\n",
    "train_images = train_images.reshape((50000, 32 * 32 * 3)) # RGB 컬러라서 x3 해줘야됨\n",
    "train_images = train_images.astype('float32') / 255 # 0에서 1사이로 스케일링을 하기 위해 /255\n",
    "test_images = test_images.reshape((10000, 32 * 32 * 3)) # RGB 컬러라서 x3 해줘야됨\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# 레이블을 범주형으로 인코딩\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# 모델 정의하기 (여기에서는 Sequential 클래스 사용) // 층쌓는것을 설계하는 부분\n",
    "model = models.Sequential() # 완전 연결 계층 모델\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(32 * 32* 3,))) # 1층 : 100개의 노드\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(100,))) # 2층 : 50개의 노드\n",
    "model.add(layers.Dense(10, activation='softmax')) # 출력층 : 10개의 노드\n",
    "\n",
    "# 모델 컴파일 하기( optimizer 부분)\n",
    "model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy', # 교차 엔트로피\n",
    "                metrics=['accuracy']) # 정확도를 기준으로 compile을 하겠다.\n",
    "\n",
    "# fit() 메서드로 모델 훈련 시키기\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128, shuffle = True) # verbose =0 을 쓰면 진행과정이 안나타남.\n",
    "# 설명 : shuffle = True이면 훈련 스텝마다 배치를 훈련 데이터에서 중복을 허용하지 않고 랜덤하게 추출합니다.\n",
    "\n",
    "# 테스트 데이터로 정확도 측정하기(모델을 평가하는 부분 // 10000장을 한꺼번에 집어넣음.)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test_acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 이미지를 keras를 이용해서 cnn신경망으로 학습시키는 코드\n",
    "\n",
    "책 : 미술관에 GAN 딥러닝 실전 프로젝트(2장 : CNN 코드)\n",
    "\n",
    "https://github.com/rickiepark/GDL_code/blob/master/02_03_deep_learning_conv_neural_network.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import 하는 코드\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K  # 백엔드가 텐서플로우로 되어있어서 \n",
    "                           # 텐서플로우 명령어 필요할 때 tf 대신에 k를 쓰겠다라는 의미.\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적재\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 0~1 사이로 데이터를 정규화한다.\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 라벨을 10개의 one hot encoding으로 생성한다.\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "\n",
    "\n",
    "# 진짜 모델 만들기\n",
    "\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 완전 연결 계층\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(128)(x) # 노드수 128개\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x) # 드롭아웃 rate = 0.5\n",
    "\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convoluyion층 4개, 완전 연결계층 2개인 6층 신경망 \n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train\n",
    "          , y_train\n",
    "          , batch_size=32\n",
    "          , epochs=10\n",
    "          , shuffle=True\n",
    "          , validation_data = (x_test, y_test))\n",
    "\n",
    "\n",
    "model.evaluate(x_test, y_test, batch_size=1000)\n",
    "\n",
    "# 맨 아래에 아래의 코드를 추가 합니다.\n",
    "model.save_weights('d:\\\\data\\\\cifar10_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제119. 위의 코드를 GPU 서버에서 돌리고 생성한 가중치가 들어있는 모델을 내려서 GPU 팀원들에게 배포하시오~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맨 아래에 아래의 코드를 추가 합니다.\n",
    "model.save_weights('d:\\\\data\\\\cifar10_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
