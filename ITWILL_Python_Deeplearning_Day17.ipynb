{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제107. 이미지 이름앞에 절대 경로가 아래처럼 붙게 하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train100'\n",
    "    \n",
    "def image_load(path):\n",
    "    file_list = os.listdir(path)\n",
    "    file_name = []\n",
    "    for i in file_list:\n",
    "        a = int(re.sub('[^0-9]', '', i)) #숫자가 아닌것은 ''으로 처리\n",
    "        file_name.append(a)\n",
    "        file_name.sort()\n",
    "    file_res = []\n",
    "    for j in file_name:\n",
    "        file_res.append('%s\\\\%d.png' %(path,j))\n",
    "    \n",
    "    return file_res\n",
    "\n",
    "print(image_load(train_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제108. cv2.imread 함수를 이용해서 이미지를 숫자로 변경하시오"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아나콘다 프롬프트창에서\n",
    "\n",
    "activate snowdeer_env  \n",
    "conda install opencv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 44  64  62]\n",
      "  [ 26  50  50]\n",
      "  [ 19  44  46]\n",
      "  ...\n",
      "  [ 69 172 167]\n",
      "  [ 76 184 183]\n",
      "  [ 72 136 137]]\n",
      "\n",
      " [[ 37  65  63]\n",
      "  [ 26  53  55]\n",
      "  [ 27  50  52]\n",
      "  ...\n",
      "  [ 61 169 163]\n",
      "  [ 75 174 171]\n",
      "  [ 77 146 145]]\n",
      "\n",
      " [[ 36  62  58]\n",
      "  [ 37  66  64]\n",
      "  [ 37  60  56]\n",
      "  ...\n",
      "  [ 62 155 153]\n",
      "  [ 64 154 150]\n",
      "  [ 57 128 123]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 99 135 172]\n",
      "  [ 84 110 143]\n",
      "  [ 42  56 130]\n",
      "  ...\n",
      "  [ 56  75  94]\n",
      "  [ 86 108 141]\n",
      "  [ 81 105 139]]\n",
      "\n",
      " [[117 146 183]\n",
      "  [ 95 118 150]\n",
      "  [ 44  64  80]\n",
      "  ...\n",
      "  [ 60  72  81]\n",
      "  [ 98 118 135]\n",
      "  [110 125 143]]\n",
      "\n",
      " [[144 174 209]\n",
      "  [123 151 182]\n",
      "  [ 83 109 139]\n",
      "  ...\n",
      "  [ 47  54  59]\n",
      "  [111 119 130]\n",
      "  [160 156 169]]]\n"
     ]
    }
   ],
   "source": [
    "import  os\n",
    "import  re\n",
    "import  cv2\n",
    "import  numpy  as  np\n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train100'\n",
    "\n",
    "def  image_load(path):\n",
    "    file_list = os.listdir(path)\n",
    "    file_name = []\n",
    "    for  i  in  file_list:\n",
    "        a = int(  re.sub('[^0-9]', '', i )  ) # 숫자가 아닌것은 '' 로 처리 \n",
    "        file_name.append(a) \n",
    "    file_name.sort()\n",
    "\n",
    "    file_res = [] \n",
    "    for  j  in   file_name:\n",
    "        file_res.append('%s\\\\%d.png' %(path,j)  )\n",
    "\n",
    "    image = []\n",
    "    for  k  in  file_res:\n",
    "        img = cv2.imread(k)\n",
    "        image.append(img)\n",
    "\n",
    "    return np.array(image)\n",
    "   \n",
    "print ( image_load( train_image) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제109. 훈련이미지의 라벨을 one hot encoding 하는 코드 구현 하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = 'D:\\\\data\\\\cifar10\\\\train_label.csv'\n",
    "import   csv\n",
    "\n",
    "def  label_load( path ):\n",
    "    file = open(path)\n",
    "    labeldata = csv.reader(file)\n",
    "    labellist = []\n",
    "    for  i   in  labeldata:\n",
    "        labellist.append(i)\n",
    "    return  labellist \n",
    "\n",
    "print ( label_load(train_label) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제110. 위의 결과가 문자가 아니라 숫자로 출력되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " ...\n",
      " [9]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "train_label = 'D:\\\\data\\\\cifar10\\\\train_label.csv'\n",
    "import   csv\n",
    "\n",
    "def  label_load( path ):\n",
    "    file = open(path)\n",
    "    labeldata = csv.reader(file)\n",
    "    labellist = []\n",
    "    for  i   in  labeldata:\n",
    "        labellist.append(i)\n",
    "\n",
    "    label = np.array(labellist)\n",
    "    label = label.astype(int)  # 숫자로 변환 \n",
    "    return  label\n",
    "\n",
    "print ( label_load(train_label) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제111. 아래의 결과를 출력하시오!\n",
    "\n",
    "[0 0 0 0 1 0 0 0 0 0 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.eye(10)[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제112. 위의 np.eye를 가지고 문제 10번에서 출력된 숫자들이 아래와 같이 one hot encoding 된 숫자로 출력되게하시오.\n",
    "\n",
    "\n",
    "[ 0 0 0 0 1 0 0 0 0 0 ]  \n",
    "[ 0 0 1 0 0 0 0 0 0 0 ]  \n",
    "[ 0 0 0 0 0 1 0 0 0 0 ]  \n",
    ".."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def label_load(path):\n",
    "    file = open(path)\n",
    "    labeldata = csv.reader(file)\n",
    "    labellist = []\n",
    "    for i in labeldata:\n",
    "        labellist.append(i)\n",
    "    label = np.array(labellist)\n",
    "    label = label.astype(int) # 숫자로 변환\n",
    "    label = np.eye(10)[label] # 열이 10개인 np.array를 만들어서 label의 값을 index로 가져서 그 위치를 1로 바꿈.\n",
    "    return label\n",
    "\n",
    "train_label = 'D:\\\\data\\\\cifar10\\\\train_label.csv'\n",
    "print(label_load(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제113. 위의 결과는 3차원인데 신경망에서 라벨을 사용하려면 2차원이어야 하므로 2차원으로 변경하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder('float', [None, 10]) # one hot encoding 된 행렬을 담는 변수\n",
    "\n",
    "# (60000, 1, 10) <--- 60000만장 1행 10열 \n",
    "\n",
    "#(60000,10) <--- 신경망에 넣기위해서 이렇게 2차원으로 만들어 주어야됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.random.randn(60000,1,10)\n",
    "y2 = y.reshape(-1,10)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제114. 2차원 데이터로 출력되겠금 label_load 함수를 수정하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def label_load(path):\n",
    "    file = open(path)\n",
    "    labeldata = csv.reader(file)\n",
    "    labellist = []\n",
    "    for i in labeldata:\n",
    "        labellist.append(i)\n",
    "    label = np.array(labellist)\n",
    "    label = label.astype(int) # 숫자로 변환\n",
    "    label = np.eye(10)[label] # 열이 10개인 np.array를 만들어서 label의 값을 index로 가져서 그 위치를 1로 바꿈.\n",
    "    label = label.reshape(-1,10)\n",
    "    return label\n",
    "\n",
    "train_label = 'D:\\\\data\\\\cifar10\\\\train_label.csv'\n",
    "print(label_load(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 데이터를 신경망에 로드하기 위해 만든 함수 2가지\n",
    "\n",
    "1. image_load\n",
    "2. label_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제115. 지금까지 만든 두개의 함수 image_load와 label_load를 loader2.py 라는 파이썬 코드에 저장하고 아래와 같이 loader2.py 를 import 한 후에 cifar10 전체 데이터를 로드하는 코드를 구현하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader2\n",
    "\n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train\\\\'\n",
    "train_label = 'D:\\\\data\\\\cifar10\\\\train_label.csv'\n",
    "\n",
    "test_image = 'D:\\\\data\\\\cifar10\\\\test\\\\'\n",
    "test_label = 'D:\\\\data\\\\cifar10\\\\test_label.csv'\n",
    "\n",
    "trainX = loader2.image_load(train_image)\n",
    "trainY = loader2.label_load(train_label)\n",
    "testX = loader2.image_load(test_image)\n",
    "testY = loader2.label_load(test_label)\n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)\n",
    "\n",
    "\"\"\"\n",
    "(50000, 32, 32, 3)\n",
    "(50000, 10)\n",
    "(10000, 32, 32, 3)\n",
    "(10000, 10)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 데이터를 신경망에 로드하기 위해 만든 함수 2가지\n",
    "\n",
    "1. image_load\n",
    "2. label_load\n",
    "3. next_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제116. 신경망에 100장씩 데이터를 로드할 수 있도록 아래와 같이 next_batch 함수를 생성하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch( data1, data2, init, fina ):\n",
    "    return data1[ init : fina ], data2[ init : fina ]\n",
    "\n",
    "test_image = 'D:\\\\data\\\\cifar10\\\\test\\\\'\n",
    "test_label = \"D:\\\\data\\\\cifar10\\\\test_label.csv\"\n",
    "\n",
    "testX = loader2.image_load(test_image)\n",
    "testY = loader2.label_load(test_label)\n",
    "\n",
    "print(next_batch( testX, testY, 0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제117. 아래와 같이 shuffle_batch 함수를 만들고 loader2.py에 추가 시키시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(data_list, label):\n",
    "    x = np.arange(len(data_list))\n",
    "    random.shuffle(x)\n",
    "    data_list2 = data_list[x]\n",
    "    label2 = label[x]\n",
    "    \n",
    "    return data_list2, label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행방법:\n",
    "import loader2\n",
    "\n",
    "test_image = 'D:\\\\data\\\\cifar10\\\\test\\\\'\n",
    "test_label = 'D:\\\\data\\\\cifar10\\\\test_label.csv'\n",
    "\n",
    "testX = loader2.image_load(test_image)\n",
    "testY = loader2.label_load(test_label)\n",
    "\n",
    "print(loader2.shuffle_batch(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 데이터를 로드하는 4개의 함수를 이용해서 vgg6 신경망으로 학습하는 전체 코드를 실행하시오 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import loader2\n",
    "\n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train\\\\'\n",
    "train_label = 'D:\\\\data\\\\cifar10\\\\train_label.csv'\n",
    "test_image = 'D:\\\\data\\\\cifar10\\\\test\\\\'\n",
    "test_label = 'D:\\\\data\\\\cifar10\\\\test_label.csv'\n",
    "\n",
    "print(\"LOADING DATA\")\n",
    "\n",
    "trainX = loader2.image_load(train_image)\n",
    "trainY = loader2.label_load(train_label)\n",
    "testX = loader2.image_load(test_image)\n",
    "testY = loader2.label_load(test_label)\n",
    "\n",
    "print(\"LOADED DATA\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#입력층\n",
    "x = tf.placeholder(\"float\",[None,32,32,3])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "#conv_1\n",
    "b1 = tf.Variable(tf.ones([128]))\n",
    "W1 = tf.Variable(tf.random_normal([3,3,3,128],stddev = 0.01))\n",
    "y1 = tf.nn.conv2d(x, W1, strides=[1,1,1,1], padding = 'SAME')\n",
    "y1 = y1 + b1\n",
    "y1 = tf.contrib.layers.batch_norm(y1,scale=True)\n",
    "y1 = tf.nn.leaky_relu(y1)\n",
    "\n",
    "#conv_2\n",
    "b1_2 = tf.Variable(tf.ones([128]))\n",
    "W1_2 =  tf.Variable(tf.random_normal([3,3,128,128],stddev = 0.01))\n",
    "y1_2 = tf.nn.conv2d(y1, W1_2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y1_2 = y1_2 + b1_2\n",
    "y1_2 = tf.contrib.layers.batch_norm(y1_2,scale=True)\n",
    "y1_2 = tf.nn.leaky_relu(y1_2)\n",
    "y1_2 = tf.nn.dropout(y1_2, keep_prob)\n",
    "\n",
    "#maxpooling\n",
    "y1_2 = tf.nn.max_pool(y1_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "\n",
    "#conv_3\n",
    "b2 = tf.Variable(tf.ones([256]))\n",
    "W2 = tf.Variable(tf.random_normal([3,3,128,256],stddev = 0.01))\n",
    "y2 = tf.nn.conv2d(y1_2, W2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y2 = y2 + b2\n",
    "y2 = tf.contrib.layers.batch_norm(y2,scale=True)\n",
    "y2 = tf.nn.leaky_relu(y2)\n",
    "\n",
    "#conv_4\n",
    "b2_2 = tf.Variable(tf.ones([256]))\n",
    "W2_2 = tf.Variable(tf.random_normal([3,3,256,256],stddev = 0.01))\n",
    "y2_2 = tf.nn.conv2d(y2, W2_2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y2_2 = y2_2 + b2_2\n",
    "y2_2 = tf.contrib.layers.batch_norm(y2_2,scale=True)\n",
    "y2_2 = tf.nn.leaky_relu(y2_2)\n",
    "y2_2 = tf.nn.dropout(y2_2, keep_prob)\n",
    "\n",
    "#maxpooling\n",
    "y2_2 = tf.nn.max_pool(y2_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "\n",
    "#conv_5\n",
    "b3 = tf.Variable(tf.ones([512]))\n",
    "W3 = tf.Variable(tf.random_normal([3,3,256,512],stddev = 0.01))\n",
    "y3 = tf.nn.conv2d(y2_2, W3, strides=[1,1,1,1], padding = 'SAME')\n",
    "y3 = y3 + b3\n",
    "y3 = tf.contrib.layers.batch_norm(y3,scale=True)\n",
    "y3 = tf.nn.leaky_relu(y3)\n",
    "\n",
    "#conv_6\n",
    "b3_2 = tf.Variable(tf.ones([512]))\n",
    "W3_2 = tf.Variable(tf.random_normal([3,3,512,512],stddev = 0.01))\n",
    "y3_2 = tf.nn.conv2d(y3, W3_2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y3_2 = y3_2 + b3_2\n",
    "y3_2 = tf.contrib.layers.batch_norm(y3_2,scale=True)\n",
    "y3_2 = tf.nn.leaky_relu(y3_2)\n",
    "y3_2 = tf.nn.dropout(y3_2, keep_prob)\n",
    "\n",
    "#maxpooling\n",
    "y3_2 = tf.nn.max_pool(y3_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "\n",
    "#Affine1\n",
    "b4 = tf.Variable(tf.ones([1024]))\n",
    "W4 = tf.get_variable(name='W4', shape=[4*4*512, 1024], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "y4 = tf.reshape(y3_2, [-1, 4*4*512])\n",
    "y4 = tf.matmul(y4,W4) + b4\n",
    "y4 = tf.contrib.layers.batch_norm(y4,scale=True)\n",
    "y4 = tf.nn.leaky_relu(y4)\n",
    "\n",
    "\n",
    "#Affine2\n",
    "b5 = tf.Variable(tf.ones([1024]))\n",
    "W5 = tf.get_variable(name='W5', shape=[1024, 1024], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "y5 = tf.matmul(y4,W5) + b5\n",
    "y5 = tf.contrib.layers.batch_norm(y5,scale=True)\n",
    "y5 = tf.nn.leaky_relu(y5)\n",
    "\n",
    "\n",
    "#드롭아웃\n",
    "y5_drop = tf.nn.dropout(y5, keep_prob)\n",
    "\n",
    "\n",
    "#출력층\n",
    "b6 = tf.Variable(tf.ones([10]))\n",
    "W6 = tf.get_variable(name='W6', shape=[1024, 10], initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값\n",
    "y6 = tf.matmul(y5_drop,W6) + b6\n",
    "y6 = tf.contrib.layers.batch_norm(y6,scale=True)\n",
    "y_hat = tf.nn.softmax(y6)\n",
    "\n",
    "\n",
    "#예측값\n",
    "y_predict = tf.argmax(y_hat,1)\n",
    "\n",
    "\n",
    "# 라벨을 저장하기 위한 변수 생성\n",
    "y_onehot = tf.placeholder(\"float\",[None,10])\n",
    "y_label = tf.argmax(y_onehot, axis = 1)\n",
    "\n",
    "\n",
    "# 정확도를 출력하기 위한 변수 생성\n",
    "correct_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "\n",
    "\n",
    "# 교차 엔트로피 오차 함수\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(y_hat), axis = 1)\n",
    "\n",
    "\n",
    "# SGD 경사 감소법\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "\n",
    "\n",
    "# Adam 경사 감소법\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "\n",
    "# 학습 오퍼레이션 정의\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for j in range(20):\n",
    "        for i in range(600):\n",
    "            trainX , trainY = loader2.shuffle_batch(trainX, trainY)\n",
    "            testX, testY = loader2.shuffle_batch(testX, testY)\n",
    "\n",
    "            train_xs, train_ys = loader2.next_batch(trainX, trainY, 0, 100)\n",
    "            test_xs, test_ys = loader2.next_batch(testX, testY, 0, 100)\n",
    "\n",
    "            sess.run(train, feed_dict={x: train_xs, y_onehot: train_ys, keep_prob: 0.8})\n",
    "\n",
    "            if i == 0:\n",
    "                train_acc = sess.run(accuracy, feed_dict={x: train_xs, y_onehot: train_ys, keep_prob: 1.0})\n",
    "                test_acc = sess.run(accuracy, feed_dict={x: test_xs, y_onehot: test_ys, keep_prob: 1.0})\n",
    "\n",
    "                train_acc_list.append(train_acc)\n",
    "                test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ gpu 피씨에서 tensor_gpu 설치\n",
    "\n",
    "1. 먼저 기존에 설치된 tensorflow를 삭제하시오\n",
    "\n",
    "__아나콘다 프롬프트 창을 열고__\n",
    "\n",
    "conda remove tensorflow\n",
    "\n",
    "2. tensorflow-gpy를 설치하시오\n",
    "\n",
    "conda install tensorflow-gpu\n",
    "\n",
    "3. cuda를 설치하시오 ~\n",
    "\n",
    "https://replaydeveloper.tistory.com/entry/Tensorflow-14-개발-환경-설치Windows-10-CUDA-80-cuDNN-v60-GPU-버전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 현재 코드는 배치정규화가 제대로 안되는 문제가 있으므로 수정하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import loader2\n",
    "\n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train\\\\'\n",
    "train_label = 'D:\\\\data\\\\cifar10\\\\train_label.csv'\n",
    "test_image = 'D:\\\\data\\\\cifar10\\\\test\\\\'\n",
    "test_label = 'D:\\\\data\\\\cifar10\\\\test_label.csv'\n",
    "\n",
    "print(\"LOADING DATA\")\n",
    "\n",
    "trainX = loader2.image_load(train_image)\n",
    "trainY = loader2.label_load(train_label)\n",
    "testX = loader2.image_load(test_image)\n",
    "testY = loader2.label_load(test_label)\n",
    "\n",
    "print(\"LOADED DATA\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#입력층\n",
    "x = tf.placeholder(\"float\",[None,32,32,3])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "training = tf.placeholder(tf.bool, name = 'training') \n",
    "# 배치정규화가 훈련할때는 True, Test할때는 False로 설정하는 코드\n",
    "# True 아니면 False가 들어가는 코드\n",
    "\n",
    "#conv_1\n",
    "b1 = tf.Variable(tf.ones([128]))\n",
    "W1 = tf.Variable(tf.random_normal([3,3,3,128],stddev = 0.01))\n",
    "y1 = tf.nn.conv2d(x, W1, strides=[1,1,1,1], padding = 'SAME')\n",
    "y1 = y1 + b1\n",
    "y1 = tf.contrib.layers.batch_norm(y1,scale=True, is_training = training)\n",
    "y1 = tf.nn.leaky_relu(y1)\n",
    "\n",
    "#conv_2\n",
    "b1_2 = tf.Variable(tf.ones([128]))\n",
    "W1_2 =  tf.Variable(tf.random_normal([3,3,128,128],stddev = 0.01))\n",
    "y1_2 = tf.nn.conv2d(y1, W1_2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y1_2 = y1_2 + b1_2\n",
    "y1_2 = tf.contrib.layers.batch_norm(y1_2,scale=True, is_training = training)\n",
    "y1_2 = tf.nn.leaky_relu(y1_2)\n",
    "y1_2 = tf.nn.dropout(y1_2, keep_prob)\n",
    "\n",
    "#maxpooling\n",
    "y1_2 = tf.nn.max_pool(y1_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "\n",
    "#conv_3\n",
    "b2 = tf.Variable(tf.ones([256]))\n",
    "W2 = tf.Variable(tf.random_normal([3,3,128,256],stddev = 0.01))\n",
    "y2 = tf.nn.conv2d(y1_2, W2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y2 = y2 + b2\n",
    "y2 = tf.contrib.layers.batch_norm(y2,scale=True, is_training = training)\n",
    "y2 = tf.nn.leaky_relu(y2)\n",
    "\n",
    "#conv_4\n",
    "b2_2 = tf.Variable(tf.ones([256]))\n",
    "W2_2 = tf.Variable(tf.random_normal([3,3,256,256],stddev = 0.01))\n",
    "y2_2 = tf.nn.conv2d(y2, W2_2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y2_2 = y2_2 + b2_2\n",
    "y2_2 = tf.contrib.layers.batch_norm(y2_2,scale=True, is_training = training)\n",
    "y2_2 = tf.nn.leaky_relu(y2_2)\n",
    "y2_2 = tf.nn.dropout(y2_2, keep_prob)\n",
    "\n",
    "#maxpooling\n",
    "y2_2 = tf.nn.max_pool(y2_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "\n",
    "#conv_5\n",
    "b3 = tf.Variable(tf.ones([512]))\n",
    "W3 = tf.Variable(tf.random_normal([3,3,256,512],stddev = 0.01))\n",
    "y3 = tf.nn.conv2d(y2_2, W3, strides=[1,1,1,1], padding = 'SAME')\n",
    "y3 = y3 + b3\n",
    "y3 = tf.contrib.layers.batch_norm(y3,scale=True, is_training = training)\n",
    "y3 = tf.nn.leaky_relu(y3)\n",
    "\n",
    "#conv_6\n",
    "b3_2 = tf.Variable(tf.ones([512]))\n",
    "W3_2 = tf.Variable(tf.random_normal([3,3,512,512],stddev = 0.01))\n",
    "y3_2 = tf.nn.conv2d(y3, W3_2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y3_2 = y3_2 + b3_2\n",
    "y3_2 = tf.contrib.layers.batch_norm(y3_2,scale=True, is_training = training)\n",
    "y3_2 = tf.nn.leaky_relu(y3_2)\n",
    "y3_2 = tf.nn.dropout(y3_2, keep_prob)\n",
    "\n",
    "#maxpooling\n",
    "y3_2 = tf.nn.max_pool(y3_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "\n",
    "#Affine1\n",
    "b4 = tf.Variable(tf.ones([1024]))\n",
    "W4 = tf.get_variable(name='W4', shape=[4*4*512, 1024], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "y4 = tf.reshape(y3_2, [-1, 4*4*512])\n",
    "y4 = tf.matmul(y4,W4) + b4\n",
    "y4 = tf.contrib.layers.batch_norm(y4,scale=True, is_training = training)\n",
    "y4 = tf.nn.leaky_relu(y4)\n",
    "\n",
    "\n",
    "#Affine2\n",
    "b5 = tf.Variable(tf.ones([1024]))\n",
    "W5 = tf.get_variable(name='W5', shape=[1024, 1024], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "y5 = tf.matmul(y4,W5) + b5\n",
    "y5 = tf.contrib.layers.batch_norm(y5,scale=True, is_training = training)\n",
    "y5 = tf.nn.leaky_relu(y5)\n",
    "\n",
    "\n",
    "#드롭아웃\n",
    "y5_drop = tf.nn.dropout(y5, keep_prob)\n",
    "\n",
    "\n",
    "#출력층\n",
    "b6 = tf.Variable(tf.ones([10]))\n",
    "W6 = tf.get_variable(name='W6', shape=[1024, 10], initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값\n",
    "y6 = tf.matmul(y5_drop,W6) + b6\n",
    "y6 = tf.contrib.layers.batch_norm(y6,scale=True, is_training = training)\n",
    "y_hat = tf.nn.softmax(y6)\n",
    "\n",
    "\n",
    "#예측값\n",
    "y_predict = tf.argmax(y_hat,1)\n",
    "\n",
    "\n",
    "# 라벨을 저장하기 위한 변수 생성\n",
    "y_onehot = tf.placeholder(\"float\",[None,10])\n",
    "y_label = tf.argmax(y_onehot, axis = 1)\n",
    "\n",
    "\n",
    "# 정확도를 출력하기 위한 변수 생성\n",
    "correct_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "\n",
    "\n",
    "# 교차 엔트로피 오차 함수\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(y_hat), axis = 1)\n",
    "\n",
    "\n",
    "# SGD 경사 감소법\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "\n",
    "\n",
    "# Adam 경사 감소법\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    # Ensures that we execute the update_ops before performing the train_step\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    \n",
    "# 학습 오퍼레이션 정의\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for j in range(20):\n",
    "        for i in range(600):\n",
    "            trainX , trainY = loader2.shuffle_batch(trainX, trainY)\n",
    "            testX, testY = loader2.shuffle_batch(testX, testY)\n",
    "\n",
    "            train_xs, train_ys = loader2.next_batch(trainX, trainY, 0, 100)\n",
    "            test_xs, test_ys = loader2.next_batch(testX, testY, 0, 100)\n",
    "\n",
    "            sess.run(train, feed_dict={x: train_xs, y_onehot: train_ys, keep_prob: 0.8, training: True})\n",
    "\n",
    "            if i == 0:\n",
    "                train_acc = sess.run(accuracy, feed_dict={x: train_xs, y_onehot: train_ys, keep_prob: 1.0, training : True})\n",
    "                test_acc = sess.run(accuracy, feed_dict={x: test_xs, y_onehot: test_ys, keep_prob: 1.0, training : False})\n",
    "                # 훈련되어진 베타와 감마가 test에 사용되려면 training을 false로 설정해야된다.\n",
    "\n",
    "                train_acc_list.append(train_acc)\n",
    "                test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ※ 배치 정규화 할때 주의할사항\n",
    "\n",
    "아래의 코드를 추가해야 훈련할 때 만든 배치정규화의 감마와 베타값을 테스트할 때 사용할 수 있게 된다.\n",
    "\n",
    "https://neurowhai.tistory.com/119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  \n",
    "with tf.control_dependencies(update_ops):  \n",
    "    # Ensures that we execute the update_ops before performing the train_step  \n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)  \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
