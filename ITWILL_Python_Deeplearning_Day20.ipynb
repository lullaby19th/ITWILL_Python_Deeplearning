{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 케글에 개/고양이 상위 6% 안에 드는 것을 목표로 신경망을 구현해보자~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. cifar10 keras 신경망( 책: 미술관에간 딥러닝 책 2장 코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import 하는 코드 \n",
    "import numpy as np\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "# to_categorical --> one hot encoding 하는 코드 \n",
    "# 2 -> [0 0 1 0 0 0 0 0 0 0 0]\n",
    "\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                            # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                            # k 를 쓰겠다라는 의미 \n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적제 \n",
    "NUM_CLASSES = 10 # 클래스 갯수\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) # (50000, 32, 32, 3)\n",
    "print(y_train.shape) # (50000, 1)\n",
    "\n",
    "#  0 ~ 1사이로 정규화 한다. \n",
    "# 하나의 픽셀(0~255)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 라벨을 10개의 one encoding 생성한다. \n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "# to_categorical = np.eyes기능과 유사함.\n",
    "# y_train의 행렬 데이터를 (50000,1) -----> (50000,NUM_CLASSES) = (50000,10)\n",
    "print(y_train.shape) # (50000,10)\n",
    "\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "# y_test의 행렬 데이터를 (10000,1) --> (10000,10)\n",
    "print(y_test.shape)\n",
    "\n",
    "# 모델 만들기 \n",
    "input_layer = Input((32,32,3)) # 뒤에 batch_size를 넣으므로 1장-> 3차원 데이터를 넣어도 됨.\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "# 필터개수 32개, 필터(kernel) 사이즈 3x3\n",
    "x = BatchNormalization()(x) # 가중치의 정규성을 강제화 시키는 기능 = 배치 정규화\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "# 완전 연결 계층 \n",
    "x = Flatten()(x)\n",
    "print(x.shape)\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "# 마지막 출력층\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x) # (100,10)\n",
    "# output_layer의 열(10)과 y_train의 열(10)이 같아야 오류가 안뜸.\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convolution 층 4개, 완전 연결계층 2개인 6층 신경망 \n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "                  # ='binary_crossentropy' (2개만 분류할꺼면 이걸로 써줘도 됨.)\n",
    "    \n",
    "model.fit(x_train\n",
    "          , y_train\n",
    "          , batch_size=100 # 100장씩 트레이닝 하는 상황.(미니배치)\n",
    "          , epochs=10\n",
    "          , shuffle=True\n",
    "          , validation_data = (x_test, y_test)) \n",
    "# 훈련데이터를 100개씩 넣으면서, 테스트데이터도 100개씩 넣으면서 확인함.\n",
    "# 오버피팅 확인하려고 저렇게 함.\n",
    "\n",
    "# 훈련이 끝난 신경망에 테스트 데이터를 입력함으로써 신경망 평가를 함,\n",
    "model.evaluate(x_test, y_test, batch_size=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip. 오차가 안줄고 정확도만 오르고 있는 상황이라면 --> Local minima에 빠져있는 상태   \n",
    "Tip. 오차가 줄어드는 것에 포커스를 맞추고 관찰하여야함.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제1. 위의 신경망에서 softmax의 확률백터를 출력하시오!\n",
    "\n",
    "[0.001 0.02 ...... 0.2] <--- 이미지 한장에 대해서 10개 출력(cifar10의 경우)\n",
    "\n",
    "[0.1 0.9] <--- 이미지 한장에 대해서 2개 출력(개/고양이일 경우)\n",
    "\n",
    "### ● 케글에 제출할 때는 테스트 데이터에 대해서 확률백터의 한쪽 확률을 제출해야 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  위에 코드 마지막에 preds = model.predict(x_test) 를 마지막에 추가해 주면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 설명 : preds 는 [10000,10] 크기의 배열입니다. 즉 샘플마다 10개의 클래스 확률을 담은 벡터가 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import 하는 코드 \n",
    "# 현재 코딩시간을 감소하여 빨리 확인하려고 1000장씩 줄인상황.\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "# to_categorical --> one hot encoding 하는 코드 \n",
    "# 2 -> [0 0 1 0 0 0 0 0 0 0 0]\n",
    "\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                            # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                            # k 를 쓰겠다라는 의미 \n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적제 \n",
    "NUM_CLASSES = 10 # 클래스 갯수\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) # (50000, 32, 32, 3)\n",
    "print(y_train.shape) # (50000, 1)\n",
    "\n",
    "#  0 ~ 1사이로 정규화 한다. \n",
    "# 하나의 픽셀(0~255)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 라벨을 10개의 one encoding 생성한다. \n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "# to_categorical = np.eyes기능과 유사함.\n",
    "# y_train의 행렬 데이터를 (50000,1) -----> (50000,NUM_CLASSES) = (50000,10)\n",
    "print(y_train.shape) # (50000,10)\n",
    "\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "# y_test의 행렬 데이터를 (10000,1) --> (10000,10)\n",
    "print(y_test.shape)\n",
    "\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "# 모델 만들기 \n",
    "input_layer = Input((32,32,3)) # 뒤에 batch_size를 넣으므로 1장-> 3차원 데이터를 넣어도 됨.\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "# 필터개수 32개, 필터(kernel) 사이즈 3x3\n",
    "x = BatchNormalization()(x) # 가중치의 정규성을 강제화 시키는 기능 = 배치 정규화\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "# 완전 연결 계층 \n",
    "x = Flatten()(x)\n",
    "print(x.shape)\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "# 마지막 출력층\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x) # (100,10)\n",
    "# output_layer의 열(10)과 y_train의 열(10)이 같아야 오류가 안뜸.\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convolution 층 4개, 완전 연결계층 2개인 6층 신경망 \n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "                  # ='binary_crossentropy' (2개만 분류할꺼면 이걸로 써줘도 됨.)\n",
    "    \n",
    "model.fit(x_train\n",
    "          , y_train\n",
    "          , batch_size=100 # 100장씩 트레이닝 하는 상황.(미니배치)\n",
    "          , epochs=1\n",
    "          , shuffle=True\n",
    "          , validation_data = (x_test, y_test)) \n",
    "# 훈련데이터를 100개씩 넣으면서, 테스트데이터도 100개씩 넣으면서 확인함.\n",
    "# 오버피팅 확인하려고 저렇게 함.\n",
    "\n",
    "# 훈련이 끝난 신경망에 테스트 데이터를 입력함으로써 신경망 평가를 함,\n",
    "model.evaluate(x_test, y_test, batch_size=1000) \n",
    "\n",
    "preds = model.predict(x_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ※ 설명 : preds는 [10000,10] 크기의 배열입니다. 즉 샘플마다 10개의 클래스 확률을 담은 벡터가 반환됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제2. 위에 훈련된 신경망에서 출력된 테스트 데이터의 확률 벡터에서 가장 큰 원소의 인덱스 번호가 무엇이지 출력해서 아래의 클래스 중에 무엇으로 예측했는지 확인하시오!\n",
    "\n",
    "1. 비행기\n",
    "2. 자동차\n",
    "3. 새\n",
    "4. 고양이\n",
    "5. 사슴\n",
    "6. 개\n",
    "7. 개구리\n",
    "8. 말\n",
    "9. 배\n",
    "10. 트럭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 코드에서 맨밑에 이 코드만 추가해 주면됨.\n",
    "\n",
    "CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', '개', '개구리', '말', '배', '트럭'])\n",
    "preds = model.predict(x_test) # 일단 전체 말고 하나만 확인\n",
    "preds_value = CLASSES[np.argmax(preds, axis = -1)][2]\n",
    "actual_value = CLASSES[np.argmax(y_test, axis = -1)][2]\n",
    "print(preds_value)\n",
    "print(actual_value)\n",
    "\n",
    "# 설명 : 여기서 axis = -1 은 마지막 차원(클래스 차원)으로 배열을 압축해라는 뜻.\n",
    "# 그래서 (10000,10) ----> (10000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import 하는 코드 \n",
    "# 현재 코딩시간을 감소하여 빨리 확인하려고 1000장씩 줄인상황.\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "# to_categorical --> one hot encoding 하는 코드 \n",
    "# 2 -> [0 0 1 0 0 0 0 0 0 0 0]\n",
    "\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                            # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                            # k 를 쓰겠다라는 의미 \n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적제 \n",
    "NUM_CLASSES = 10 # 클래스 갯수\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) # (50000, 32, 32, 3)\n",
    "print(y_train.shape) # (50000, 1)\n",
    "\n",
    "#  0 ~ 1사이로 정규화 한다. \n",
    "# 하나의 픽셀(0~255)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 라벨을 10개의 one encoding 생성한다. \n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "# to_categorical = np.eyes기능과 유사함.\n",
    "# y_train의 행렬 데이터를 (50000,1) -----> (50000,NUM_CLASSES) = (50000,10)\n",
    "print(y_train.shape) # (50000,10)\n",
    "\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "# y_test의 행렬 데이터를 (10000,1) --> (10000,10)\n",
    "print(y_test.shape)\n",
    "\n",
    "# 코드만 되는지 안되는지 확인만 해볼거라 1000개씩 일단 가져옴.\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "# 모델 만들기 \n",
    "input_layer = Input((32,32,3)) # 뒤에 batch_size를 넣으므로 1장-> 3차원 데이터를 넣어도 됨.\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "# 필터개수 32개, 필터(kernel) 사이즈 3x3\n",
    "x = BatchNormalization()(x) # 가중치의 정규성을 강제화 시키는 기능 = 배치 정규화\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "# 완전 연결 계층 \n",
    "x = Flatten()(x)\n",
    "print(x.shape)\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "# 마지막 출력층\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x) # (100,10)\n",
    "# output_layer의 열(10)과 y_train의 열(10)이 같아야 오류가 안뜸.\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convolution 층 4개, 완전 연결계층 2개인 6층 신경망 \n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "                  # ='binary_crossentropy' (2개만 분류할꺼면 이걸로 써줘도 됨.)\n",
    "    \n",
    "model.fit(x_train\n",
    "          , y_train\n",
    "          , batch_size=100 # 100장씩 트레이닝 하는 상황.(미니배치)\n",
    "          , epochs=1\n",
    "          , shuffle=True\n",
    "          , validation_data = (x_test, y_test)) \n",
    "# 훈련데이터를 100개씩 넣으면서, 테스트데이터도 100개씩 넣으면서 확인함.\n",
    "# 오버피팅 확인하려고 저렇게 함.\n",
    "\n",
    "# 훈련이 끝난 신경망에 테스트 데이터를 입력함으로써 신경망 평가를 함,\n",
    "model.evaluate(x_test, y_test, batch_size=1000) \n",
    "\n",
    "\n",
    "CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', '개', '개구리', '말', '배', '트럭'])\n",
    "preds = model.predict(x_test) # 일단 전체 말고 하나만 확인\n",
    "preds_value = CLASSES[np.argmax(preds, axis = -1)][2]\n",
    "actual_value = CLASSES[np.argmax(y_test, axis = -1)][2]\n",
    "print(preds_value)\n",
    "print(actual_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 점심시간 문제 : convolution층을 하나 더넣고 fully connect 층도 하나 더 넣어서 10에폭으로 모델을 생성하고 생선된 모델을 아래의 명령어로 내리고 검사받으세요~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import 하는 코드 \n",
    "# 현재 코딩시간을 감소하여 빨리 확인하려고 1000장씩 줄인상황.\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "# to_categorical --> one hot encoding 하는 코드 \n",
    "# 2 -> [0 0 1 0 0 0 0 0 0 0 0]\n",
    "\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                            # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                            # k 를 쓰겠다라는 의미 \n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적제 \n",
    "NUM_CLASSES = 10 # 클래스 갯수\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) # (50000, 32, 32, 3)\n",
    "print(y_train.shape) # (50000, 1)\n",
    "\n",
    "#  0 ~ 1사이로 정규화 한다. \n",
    "# 하나의 픽셀(0~255)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 라벨을 10개의 one encoding 생성한다. \n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "# to_categorical = np.eyes기능과 유사함.\n",
    "# y_train의 행렬 데이터를 (50000,1) -----> (50000,NUM_CLASSES) = (50000,10)\n",
    "print(y_train.shape) # (50000,10)\n",
    "\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "# y_test의 행렬 데이터를 (10000,1) --> (10000,10)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# 모델 만들기 \n",
    "input_layer = Input((32,32,3)) # 뒤에 batch_size를 넣으므로 1장-> 3차원 데이터를 넣어도 됨.\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "# 필터개수 32개, 필터(kernel) 사이즈 3x3\n",
    "x = BatchNormalization()(x) # 가중치의 정규성을 강제화 시키는 기능 = 배치 정규화\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 추가된 convolution 계층\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 완전 연결 계층 \n",
    "x = Flatten()(x)\n",
    "print(x.shape)\n",
    "\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "# 추가된 완전 연결 계층\n",
    "x = Dense(100)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "# 마지막 출력층\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x) # (100,10)\n",
    "# output_layer의 열(10)과 y_train의 열(10)이 같아야 오류가 안뜸.\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convolution 층 4개, 완전 연결계층 2개인 6층 신경망 \n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "                  # ='binary_crossentropy' (2개만 분류할꺼면 이걸로 써줘도 됨.)\n",
    "    \n",
    "model.fit(x_train\n",
    "          , y_train\n",
    "          , batch_size=100 # 100장씩 트레이닝 하는 상황.(미니배치)\n",
    "          , epochs=10\n",
    "          , shuffle=True\n",
    "          , validation_data = (x_test, y_test)) \n",
    "# 훈련데이터를 100개씩 넣으면서, 테스트데이터도 100개씩 넣으면서 확인함.\n",
    "# 오버피팅 확인하려고 저렇게 함.\n",
    "\n",
    "# 훈련이 끝난 신경망에 테스트 데이터를 입력함으로써 신경망 평가를 함,\n",
    "model.evaluate(x_test, y_test, batch_size=1000) \n",
    "\n",
    "\n",
    "CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', '개', '개구리', '말', '배', '트럭'])\n",
    "preds = model.predict(x_test) # 일단 전체 말고 하나만 확인\n",
    "preds_value = CLASSES[np.argmax(preds, axis = -1)][2]\n",
    "actual_value = CLASSES[np.argmax(y_test, axis = -1)][2]\n",
    "\n",
    "model.save_weights('cifar10_v1.h5') # 이렇게 쓰면 working 디렉토리에 저장됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제3. 훈련이 다된 신경망에 테스트 데이터를 한번에 넣어서 평가하는 코드를 구현하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체코드에서 이 부분만 수정하면 됨.\n",
    "\n",
    "model.fit(x_train\n",
    "          , y_train\n",
    "          , batch_size=100 # 100장씩 트레이닝 하는 상황.(미니배치)\n",
    "          , epochs=10\n",
    "          , shuffle=True, validation_split = 0.25) \n",
    "\n",
    "\n",
    "# 케글에서는 test_data에 라벨이 없으므로  validation_data 부분을 뺌.(오버피팅을 확인할 수 없다.)\n",
    "# validation_split : 훈련할 때 훈련 데이터의 일부를 테스트 데이터(25%)로 쓰면서, 오버피팅이 일어나는지 확인하겠다.\n",
    "\n",
    "model.evaluate(x_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정한 전체 코드(Full code)\n",
    "\n",
    "# 필요한 라이브러리 import 하는 코드 \n",
    "# 현재 코딩시간을 감소하여 빨리 확인하려고 1000장씩 줄인상황.\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "# to_categorical --> one hot encoding 하는 코드 \n",
    "# 2 -> [0 0 1 0 0 0 0 0 0 0 0]\n",
    "\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                            # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                            # k 를 쓰겠다라는 의미 \n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적제 \n",
    "NUM_CLASSES = 10 # 클래스 갯수\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) # (50000, 32, 32, 3)\n",
    "print(y_train.shape) # (50000, 1)\n",
    "\n",
    "#  0 ~ 1사이로 정규화 한다. \n",
    "# 하나의 픽셀(0~255)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 라벨을 10개의 one encoding 생성한다. \n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "# to_categorical = np.eyes기능과 유사함.\n",
    "# y_train의 행렬 데이터를 (50000,1) -----> (50000,NUM_CLASSES) = (50000,10)\n",
    "print(y_train.shape) # (50000,10)\n",
    "\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "# y_test의 행렬 데이터를 (10000,1) --> (10000,10)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# 모델 만들기 \n",
    "input_layer = Input((32,32,3)) # 뒤에 batch_size를 넣으므로 1장-> 3차원 데이터를 넣어도 됨.\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "# 필터개수 32개, 필터(kernel) 사이즈 3x3\n",
    "x = BatchNormalization()(x) # 가중치의 정규성을 강제화 시키는 기능 = 배치 정규화\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 추가된 convolution 계층\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 완전 연결 계층 \n",
    "x = Flatten()(x)\n",
    "print(x.shape)\n",
    "\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "# 추가된 완전 연결 계층\n",
    "x = Dense(100)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "# 마지막 출력층\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x) # (100,10)\n",
    "# output_layer의 열(10)과 y_train의 열(10)이 같아야 오류가 안뜸.\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convolution 층 4개, 완전 연결계층 2개인 6층 신경망 \n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "                  # ='binary_crossentropy' (2개만 분류할꺼면 이걸로 써줘도 됨.)\n",
    "    \n",
    "model.fit(x_train\n",
    "          , y_train\n",
    "          , batch_size=100 # 100장씩 트레이닝 하는 상황.(미니배치)\n",
    "          , epochs=10\n",
    "          , shuffle=True, validation_split=0.25) \n",
    "\n",
    "# 훈련데이터를 100개씩 넣으면서, 테스트데이터도 100개씩 넣으면서 확인함.\n",
    "# 오버피팅 확인하려고 저렇게 함.\n",
    "# 훈련이 끝난 신경망에 테스트 데이터를 입력함으로써 신경망 평가를 함,\n",
    "model.evaluate(x_test, y_test, batch_size=1000) \n",
    "\n",
    "\n",
    "CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', '개', '개구리', '말', '배', '트럭'])\n",
    "preds = model.predict(x_test) # 일단 전체 말고 하나만 확인\n",
    "preds_value = CLASSES[np.argmax(preds, axis = -1)][2]\n",
    "actual_value = CLASSES[np.argmax(y_test, axis = -1)][2]\n",
    "\n",
    "print (preds_value)\n",
    "print (actual_value)\n",
    "\n",
    "model.save_weights('d:\\\\data\\\\cifar10_v1.h5') # 저장위치를 설정할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제4. 훈련 데이터 정확도 74% 의 모델 cifar10_v1.h5 모델을 불러와서 cifar10 테스트 데이터 한장을 잘 맞추는지 확인하시오!\n",
    "\n",
    "훈련은 시키지 말고 모델을 바로 불러와서 사진 한장을 잘 맞추는지 확인하시오! (아래의 코드를 참고하시오!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 힌트\n",
    "# 모델을 불러오는 코드\n",
    "self.model = self.build_model()\n",
    "    if train:\n",
    "        self.model = self.train(self.model)\n",
    "    else:\n",
    "        self.model.load_weights('D:\\\\data\\\\cifar10vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import 하는 코드 \n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                                     # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                                     # k 를 쓰겠다라는 의미 \n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적제 \n",
    "NUM_CLASSES = 10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "\n",
    "#  0 ~ 1사이로 정규화 한다. \n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "# 라벨을 10개의 one encoding 생성한다. \n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "\n",
    "\n",
    "# 모델 만들기 \n",
    "input_layer = Input((32,32,3))\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 추가된 convolution 계층\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "# 완전 연결 계층 \n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "# 추가된 완전 연결 계층\n",
    "x = Dense(100)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "\n",
    "output_layer = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convolution 층 4개, 완전 연결계층 2개인 6층 신경망 \n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "\n",
    "model_exists = True\n",
    "\n",
    "if model_exists:\n",
    "    \n",
    "    model.load_weights('D:\\\\data\\\\cifar10_v1.h5')\n",
    "    \n",
    "    CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', '개', '개구리', '말',\n",
    "                                '배', '트럭' ]) \n",
    "    preds = model.predict( x_test[1:2] )   \n",
    "    preds_value = CLASSES[ np.argmax( preds, axis= -1 ) ] \n",
    "    actual_value = CLASSES[ np.argmax( y_test [1:2], axis = -1) ] \n",
    "    print ( preds_value)\n",
    "    print ( actual_value)\n",
    "    \n",
    "else:\n",
    "        \n",
    "    opt = Adam(lr=0.0005)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train\n",
    "              , y_train\n",
    "              , batch_size=100\n",
    "              , epochs=1\n",
    "              , shuffle=True\n",
    "               , validation_split=0.25)\n",
    "    \n",
    "    \n",
    "    model.save_weights('D:\\\\data\\\\cifar10_v1.h5')\n",
    "    \n",
    "    model.evaluate(x_test, y_test, batch_size=1000)\n",
    "    \n",
    "    CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', '개', '개구리', '말',\n",
    "                                '배', '트럭' ]) \n",
    "    preds = model.predict( x_test[1:2] )   \n",
    "    preds_value = CLASSES[ np.argmax( preds, axis= -1 ) ] \n",
    "    actual_value = CLASSES[ np.argmax( y_test[1:2], axis = -1) ] \n",
    "    print ( preds_value)\n",
    "    print ( actual_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 을 vgg16으로 구성된 신경망으로 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기버전 ver1\n",
    "\n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def prep_data(images): \n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, ROWS, COLS, CHANNELS), dtype=np.uint8) # 4차원 짜리 비어있는 numpy array형을 만듬.\n",
    "    \n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file) # resize된 이미지를 image변수에 저장함\n",
    "        data[i] = image # data라는 numpy array에 image 변수값을 저장.\n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count)) # 진행경과를 확인하는 코드\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "x_train = prep_data(train_images)\n",
    "x_test = prep_data(test_images)\n",
    "\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ Kaggle 버젼으로 저장해놓은 cifar10 사진들을 가져옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간버전 ver2\n",
    "# 케글 버젼으로 저장된 cifar10 사진을 가져옴.\n",
    "\n",
    "import os, cv2, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 필요한 라이브러리 import 하는 코드 \n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                                     # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                                     # k 를 쓰겠다라는 의미 \n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "TRAIN_DIR = 'D:\\\\data\\\\cifar10\\\\train\\\\'\n",
    "TEST_DIR = 'D:\\\\data\\\\cifar10\\\\test\\\\'\n",
    "\n",
    "train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\n",
    "test_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "# slice datasets for memory efficiency on Kaggle Kernels, delete if using full dataset\n",
    "train_images = test_images[:1000] \n",
    "test_images =  test_images[:1000]\n",
    "\n",
    "\n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prep_data(images):\n",
    "    count = len(images) # 전체 사진 갯수 // 현재 : 1000개\n",
    "    data = np.ndarray((count, ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "    # 4차원 짜리 비어있는 numpy array 생성 \n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image       \n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))\n",
    "        \n",
    "    return data\n",
    "\n",
    "x_train = prep_data(train_images)\n",
    "x_test = prep_data(test_images)\n",
    "\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ keras 버전 (224,224,3)으로 바꾸어 주는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras ver 224,224,3으로 바꾸는 코드\n",
    "# keras에 내장되어있는 cifar10 데이터들을 가져옴\n",
    "\n",
    "import os, cv2, random\n",
    "\n",
    "# 필요한 라이브러리 import 하는 코드 \n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                                     # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                                     # k 를 쓰겠다라는 의미 \n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적제 \n",
    "NUM_CLASSES = 10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "train_images= x_train \n",
    "test_images = x_test \n",
    "\n",
    "\n",
    "ROWS=224\n",
    "COLS =224\n",
    "CHANNELS = 3\n",
    "\n",
    "def read_image(file_path):\n",
    "    img = file_path\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, ROWS, COLS,CHANNELS), dtype=np.uint8)\n",
    "    # 4차원 짜리 비어있는 numpy array 생성 \n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image       \n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))\n",
    "        \n",
    "    return data\n",
    "\n",
    "x_train = prep_data(train_images)\n",
    "x_test = prep_data(test_images)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 을 vgg16으로 구성된 신경망으로 학습시키는 풀코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, random\n",
    "\n",
    "# 필요한 라이브러리 import 하는 코드 \n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K   # 백엔드가  텐서 플로우로 되어있어서 \n",
    "                                     # 텐서 플로우 명령어 필요할 때 tf 대신에 \n",
    "                                     # k 를 쓰겠다라는 의미 \n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적제 \n",
    "NUM_CLASSES = 10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "train_images= x_train \n",
    "test_images = x_test \n",
    "\n",
    "\n",
    "ROWS= 224\n",
    "COLS = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "def read_image(file_path):\n",
    "    img = file_path\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, ROWS, COLS,CHANNELS), dtype=np.uint8)\n",
    "    # 4차원 짜리 비어있는 numpy array 생성 \n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image       \n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))\n",
    "        \n",
    "    return data\n",
    "\n",
    "x_train = prep_data(train_images)\n",
    "x_test = prep_data(test_images)\n",
    "\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "\n",
    "#  0 ~ 1사이로 정규화 한다. \n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "# 라벨을 10개의 one encoding 생성한다. \n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "\n",
    "\n",
    "# 모델 만들기 \n",
    "input_layer = Input((224,224,3))\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "\n",
    "# 완전 연결 계층 \n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(4096)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "x = Dense(4096)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convolution 층 4개, 완전 연결계층 2개인 6층 신경망 \n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "\n",
    "model_exists = False\n",
    "\n",
    "\n",
    "if model_exists:\n",
    "    \n",
    "    \n",
    "    model.load_weights('D:\\\\data\\\\cifar10vgg16.h5')\n",
    "    \n",
    "    CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', '개', '개구리', '말',\n",
    "                                '배', '트럭' ]) \n",
    "    preds = model.predict( x_test[1:2] )   \n",
    "    preds_value = CLASSES[ np.argmax( preds, axis= -1 ) ] \n",
    "    actual_value = CLASSES[ np.argmax( y_test [1:2], axis = -1) ] \n",
    "    print ( preds_value)\n",
    "    print ( actual_value)\n",
    "    \n",
    "else:\n",
    "        \n",
    "    opt = Adam(lr=0.0005)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train\n",
    "              , y_train\n",
    "              , batch_size=32\n",
    "              , epochs=1\n",
    "              , shuffle=True\n",
    "               , validation_data=(x_test,y_test) )\n",
    "    \n",
    "    \n",
    "    model.save_weights('D:\\\\data\\\\cifar10vgg16.h5')\n",
    "    \n",
    "    model.evaluate(x_test, y_test, batch_size=1000)\n",
    "    \n",
    "    CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', '개', '개구리', '말',\n",
    "                                '배', '트럭' ]) \n",
    "    preds = model.predict( x_test[1:2] )   \n",
    "    preds_value = CLASSES[ np.argmax( preds, axis= -1 ) ] \n",
    "    actual_value = CLASSES[ np.argmax( y_test[1:2], axis = -1) ] \n",
    "    print ( preds_value)\n",
    "    print ( actual_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 기존의 것은 1000개씩 사진들을 가져오기때문에 10개로 줄여서 돌리는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import 하는 코드\n",
    "\n",
    "import numpy as np\n",
    "import os, cv2, random\n",
    "from keras.layers import Input, MaxPooling2D, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# 데이터 적재\n",
    "NUM_CLASSES = 10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train[:10]\n",
    "y_train = y_train[:10]\n",
    "x_test = x_test[:10]\n",
    "y_test = y_test[:10]\n",
    "\n",
    "train_images= x_train\n",
    "test_images = x_test\n",
    "\n",
    "\n",
    "ROWS=224\n",
    "COLS =224\n",
    "CHANNELS = 3\n",
    "\n",
    "def read_image(file_path):\n",
    "img = file_path\n",
    "return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prep_data(images):\n",
    "count = len(images)\n",
    "data = np.ndarray((count, ROWS, COLS,CHANNELS), dtype=np.uint8)\n",
    "# 4차원 짜리 비어있는 numpy array 생성\n",
    "\n",
    "for i, image_file in enumerate(images):\n",
    "image = read_image(image_file)\n",
    "data[i] = image\n",
    "if i%250 == 0: print('Processed {} of {}'.format(i, count))\n",
    "\n",
    "return data\n",
    "\n",
    "x_train = prep_data(train_images)\n",
    "x_test = prep_data(test_images)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# 0 ~ 1사이로 정규화 한다.\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 라벨을 10개의 one encoding 생성한다.\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# 모델 만들기\n",
    "input_layer = Input((224,224,3))\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size = 2, strides = 2)(x)\n",
    "\n",
    "x = Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size = 2, strides = 2)(x)\n",
    "\n",
    "x = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size = 2, strides = 2)(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size = 2, strides = 2)(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size = 2, strides = 2)(x)\n",
    "\n",
    "# 완전 연결 계층\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(4096)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "x = Dense(4096)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate = 0.5)(x)\n",
    "\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# convolution 층 5개, 완전 연결계층 3개인 8층 신경망\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "opt = Adam(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train\n",
    ", y_train\n",
    ", batch_size=32\n",
    ", epochs=10\n",
    ", shuffle=True\n",
    ", validation_data = (x_test, y_test))\n",
    "\n",
    "model.evaluate(x_test, y_test, batch_size=1000)\n",
    "\n",
    "preds = model.predict(x_test)\n",
    "\n",
    "CLASSES = np.array(['비행기', '자동차', '새', '고양이', '사슴', \\\n",
    "'개', '개구리', '말', '배', '트럭'])\n",
    "\n",
    "preds = model.predict(x_test)\n",
    "preds_value = CLASSES[np.argmax(preds, axis = -1)][2] #axis = -1은 마지막 차원으로 배열을 압축하라는 뜻임\n",
    "actual_value = CLASSES[np.argmax(y_test, axis = -1)][2] # (10000, 10) → (10000, 1)\n",
    "\n",
    "print(preds_value)\n",
    "print(actual_value)\n",
    "\n",
    "model.save_weights('cifar10_vgg16.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
