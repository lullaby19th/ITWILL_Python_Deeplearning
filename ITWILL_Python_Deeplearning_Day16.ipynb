{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제97. 위의 코드를 수정해서 정확도를 1에폭 돌때 마다 출력되게 하시오!\n",
    "\n",
    "1 에폭 정확도 0.90  \n",
    "2 에폭 정확도 0.92  \n",
    "3 에폭 정확도 0.94  \n",
    ".  \n",
    ".  \n",
    ".  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# 1층\n",
    "x = tf.placeholder(\"float\", [None,784])\n",
    "W1 = tf.Variable(tf.random_uniform([784,50],-1,1))\n",
    "b1 = tf.Variable(tf.ones([50]))\n",
    "\n",
    "y = tf.matmul(x, W1) + b1\n",
    "y_hat = tf.nn.relu(y) # tensorflow에서는 sigmoid함수를 안만들어도 이렇게 하면 된다.\n",
    "\n",
    "# 2층 출력층\n",
    "W2 = tf.Variable(tf.random_uniform([50,10],-1,1))\n",
    "b2 = tf.Variable(tf.ones([10]))\n",
    "z = tf.matmul(y_hat, W2) + b2\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder(\"float\", [None,10])\n",
    "y_label = tf.argmax(y_onehot, axis =1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label) # 같으면 True, 다르면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction, \"float\")) # True = 1, False = 0 으로 바꿈\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.001), axis = 1)\n",
    "result = tf.reduce_mean(loss)\n",
    "# softmax를 통과한것을 tf.log부분에 넣어야 되기 때문에 z_hat이 들어간다.\n",
    "\n",
    "# 학습 시키는 코드\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer() # 변수 초기화는 딱 1번만 하면됨.\n",
    "\n",
    "# 세션 생성\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 데이터 불러오는 부분\n",
    "for i in range(1,601*20): # 1epoch = 600번\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # batch_xs -> 훈련데이터\n",
    "    # batch_ys -> 훈련데이터 라벨\n",
    "    sess.run(train, feed_dict = { x: batch_xs, y_onehot : batch_ys })\n",
    "    if i % 600 == 0:\n",
    "        print(int(i/600), '에폭 훈련 정확도', sess.run(accuracy, feed_dict = { x: batch_xs, y_onehot : batch_ys })) # 정확도 출력을 위해 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제98. underfitting 되고 있는지 확인하기 위해서   \n",
    "## 테스트 데이터에 대한 정확도도 같이 출력하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# 1층\n",
    "x = tf.placeholder(\"float\", [None,784])\n",
    "W1 = tf.Variable(tf.random_uniform([784,50],-1,1))\n",
    "b1 = tf.Variable(tf.ones([50]))\n",
    "\n",
    "y = tf.matmul(x, W1) + b1\n",
    "y_hat = tf.nn.relu(y) # tensorflow에서는 sigmoid함수를 안만들어도 이렇게 하면 된다.\n",
    "\n",
    "# 2층 출력층\n",
    "W2 = tf.Variable(tf.random_uniform([50,10],-1,1))\n",
    "b2 = tf.Variable(tf.ones([10]))\n",
    "z = tf.matmul(y_hat, W2) + b2\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder(\"float\", [None,10])\n",
    "y_label = tf.argmax(y_onehot, axis =1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label) # 같으면 True, 다르면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction, \"float\")) # True = 1, False = 0 으로 바꿈\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.001), axis = 1)\n",
    "result = tf.reduce_mean(loss)\n",
    "# softmax를 통과한것을 tf.log부분에 넣어야 되기 때문에 z_hat이 들어간다.\n",
    "\n",
    "# 학습 시키는 코드\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer() # 변수 초기화는 딱 1번만 하면됨.\n",
    "\n",
    "# 세션 생성\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 데이터 불러오는 부분\n",
    "for i in range(1,601*20): # 1epoch = 600번\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # batch_xs -> 훈련데이터\n",
    "    # batch_ys -> 훈련데이터 라벨\n",
    "    batch_xt, batch_yt = mnist.test.next_batch(100)\n",
    "    # batch_xt -> 테스트데이터\n",
    "    # batch_ys -> 테스트데이터 라벨\n",
    "    sess.run(train, feed_dict = { x: batch_xs, y_onehot : batch_ys })\n",
    "    if i % 600 == 0:\n",
    "        print(int(i/600), '에폭 훈련 정확도', sess.run(accuracy, feed_dict = { x: batch_xs, y_onehot : batch_ys })) # 정확도 출력을 위해 accuracy\n",
    "        print(int(i/600), '에폭 테스트 정확도', sess.run(accuracy, feed_dict = { x: batch_xt, y_onehot : batch_yt })) # 정확도 출력을 위해 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 언더 피팅과 오버 피팅을 막는 방법 구현\n",
    "\n",
    "1. 언더 피팅을 줄일수 있는 방법\n",
    "\n",
    "    - 가중치 초기화 설정\n",
    "        - Xavier (p.206)\n",
    "        - He (p.206)\n",
    "    - 배치 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 초기화 방법\n",
    "\n",
    "# W = tf.Variable(tf.random_uniform([784,10], -1, 1)) # [784,10] 형상을 가진 -1~1 사이의 균등분포 어레이\n",
    "\n",
    "# W = tf.get_variable(name=\"W\", shape=[784, 10], initializer=tf.contrib.layers.xavier_initializer()) # xavier 초기값\n",
    "\n",
    "# W = tf.get_variable(name='W', shape=[784, 10], initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제99. 점심시간에 생성한 신경망에 가중치 초기화 값을 Xavier로 해서 생성하고 훈련을 시킨 정확도를 확인하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# 1층\n",
    "x = tf.placeholder(\"float\", [None,784])\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[784, 50], initializer=tf.contrib.layers.xavier_initializer()) # xavier 초기값\n",
    "b1 = tf.Variable(tf.ones([50]))\n",
    "\n",
    "y = tf.matmul(x, W1) + b1\n",
    "y_hat = tf.nn.relu(y) # tensorflow에서는 sigmoid함수를 안만들어도 이렇게 하면 된다.\n",
    "\n",
    "# 2층 출력층\n",
    "\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[50, 10], initializer=tf.contrib.layers.xavier_initializer()) # xavier 초기값\n",
    "b2 = tf.Variable(tf.ones([10]))\n",
    "z = tf.matmul(y_hat, W2) + b2\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder(\"float\", [None,10])\n",
    "y_label = tf.argmax(y_onehot, axis =1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label) # 같으면 True, 다르면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction, \"float\")) # True = 1, False = 0 으로 바꿈\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.001), axis = 1)\n",
    "result = tf.reduce_mean(loss)\n",
    "# softmax를 통과한것을 tf.log부분에 넣어야 되기 때문에 z_hat이 들어간다.\n",
    "\n",
    "# 학습 시키는 코드\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer() # 변수 초기화는 딱 1번만 하면됨.\n",
    "\n",
    "# 세션 생성\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 데이터 불러오는 부분\n",
    "for i in range(1,601*20): # 1epoch = 600번\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # batch_xs -> 훈련데이터\n",
    "    # batch_ys -> 훈련데이터 라벨\n",
    "    batch_xt, batch_yt = mnist.test.next_batch(100)\n",
    "    # batch_xt -> 테스트데이터\n",
    "    # batch_ys -> 테스트데이터 라벨\n",
    "    sess.run(train, feed_dict = { x: batch_xs, y_onehot : batch_ys })\n",
    "    if i % 600 == 0:\n",
    "        print(int(i/600), '에폭 훈련 정확도', sess.run(accuracy, feed_dict = { x: batch_xs, y_onehot : batch_ys })) # 정확도 출력을 위해 accuracy\n",
    "        print(int(i/600), '에폭 테스트 정확도', sess.run(accuracy, feed_dict = { x: batch_xt, y_onehot : batch_yt })) # 정확도 출력을 위해 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 텐서 플로우로 배치 정규화 구현하는 방법\n",
    "\n",
    "### 배치정규화란?\n",
    "\n",
    "__\" 신경망 학습시 가중치의 값의 데이터가 골고루 분산될수 있도록 하는것을 강제화 하는 장치 \"__ (면접문제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현코드 : batch_z1 = tf.contrib.layers.batch_norm(z1, True)\n",
    "#                   ↓\n",
    "# Affine1 ------> 배치정규화 ------> relu\n",
    "#  (z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제100. 지금까지 완성한 신경망에 배치 정규화를 1층에 구현하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "tf.reset_default_graph() # 그래프 초기화 코드\n",
    "\n",
    "# 1층\n",
    "x = tf.placeholder(\"float\", [None,784])\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[784, 50], initializer=tf.contrib.layers.xavier_initializer()) # xavier 초기값\n",
    "b1 = tf.Variable(tf.ones([50]))\n",
    "\n",
    "y = tf.matmul(x, W1) + b1\n",
    "batch_y = tf.contrib.layers.batch_norm(y, True) # True를 킴으로써 배치정규화를 실시함.\n",
    "y_hat = tf.nn.relu(batch_y) # tensorflow에서는 sigmoid함수를 안만들어도 이렇게 하면 된다.\n",
    "\n",
    "# 2층 출력층\n",
    "\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[50, 10], initializer=tf.contrib.layers.xavier_initializer()) # xavier 초기값\n",
    "b2 = tf.Variable(tf.ones([10]))\n",
    "z = tf.matmul(y_hat, W2) + b2\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder(\"float\", [None,10])\n",
    "y_label = tf.argmax(y_onehot, axis =1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label) # 같으면 True, 다르면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction, \"float\")) # True = 1, False = 0 으로 바꿈\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.001), axis = 1)\n",
    "result = tf.reduce_mean(loss)\n",
    "# softmax를 통과한것을 tf.log부분에 넣어야 되기 때문에 z_hat이 들어간다.\n",
    "\n",
    "# 학습 시키는 코드\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer() # 변수 초기화는 딱 1번만 하면됨.\n",
    "\n",
    "# 세션 생성\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 데이터 불러오는 부분\n",
    "for i in range(1,601*20): # 1epoch = 600번\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100) # train\n",
    "    # batch_xs -> 훈련데이터\n",
    "    # batch_ys -> 훈련데이터 라벨\n",
    "    batch_xt, batch_yt = mnist.test.next_batch(100) # test\n",
    "    # batch_xt -> 테스트데이터\n",
    "    # batch_ys -> 테스트데이터 라벨\n",
    "    sess.run(train, feed_dict = { x: batch_xs, y_onehot : batch_ys })\n",
    "    if i % 600 == 0:\n",
    "        print(int(i/600), '에폭 훈련 정확도', sess.run(accuracy, feed_dict = { x: batch_xs, y_onehot : batch_ys })) # 정확도 출력을 위해 accuracy\n",
    "        print(int(i/600), '에폭 테스트 정확도', sess.run(accuracy, feed_dict = { x: batch_xt, y_onehot : batch_yt })) # 정확도 출력을 위해 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제101. 위의 2층 신경망을 3층신경망으로 변경하고 정확도를 확인하시오!\n",
    "\n",
    "입력층 -----> 1층 ------> 2층 ------> 3층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "tf.reset_default_graph() # 그래프 초기화 코드\n",
    "\n",
    "# 1층\n",
    "x = tf.placeholder(\"float\", [None,784])\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[784, 100], initializer=tf.contrib.layers.xavier_initializer()) # xavier 초기값\n",
    "b1 = tf.Variable(tf.ones([100]))\n",
    "y = tf.matmul(x, W1) + b1\n",
    "batch_y = tf.contrib.layers.batch_norm(y, True) # True를 킴으로써 배치정규화를 실시함.\n",
    "y_hat = tf.nn.relu(batch_y) # tensorflow에서는 sigmoid함수를 안만들어도 이렇게 하면 된다.\n",
    "\n",
    "# 2층\n",
    "\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[100, 50], initializer=tf.contrib.layers.xavier_initializer()) # xavier 초기값\n",
    "b2 = tf.Variable(tf.ones([50]))\n",
    "z = tf.matmul(y_hat, W2) + b2\n",
    "batch_z = tf.contrib.layers.batch_norm(z, True)\n",
    "z_hat = tf.nn.relu(batch_z)\n",
    "\n",
    "\n",
    "# 3층 출력층 \n",
    "W3 = tf.get_variable(name=\"W3\", shape=[50, 10], initializer=tf.contrib.layers.xavier_initializer()) # xavier 초기값\n",
    "b3 = tf.Variable(tf.ones([10]))\n",
    "o = tf.matmul(z_hat, W3) + b3\n",
    "o_hat = tf.nn.softmax(o)\n",
    "y_predict = tf.argmax(o_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder(\"float\", [None,10])\n",
    "y_label = tf.argmax(y_onehot, axis =1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label) # 같으면 True, 다르면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction, \"float\")) # True = 1, False = 0 으로 바꿈\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(o_hat+0.001), axis = 1)\n",
    "result = tf.reduce_mean(loss)\n",
    "# softmax를 통과한것을 tf.log부분에 넣어야 되기 때문에 z_hat이 들어간다.\n",
    "\n",
    "# 학습 시키는 코드\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer() # 변수 초기화는 딱 1번만 하면됨.\n",
    "\n",
    "# 세션 생성\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 데이터 불러오는 부분\n",
    "for i in range(1,601*20): # 1epoch = 600번\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100) # train\n",
    "    # batch_xs -> 훈련데이터\n",
    "    # batch_ys -> 훈련데이터 라벨\n",
    "    batch_xt, batch_yt = mnist.test.next_batch(100) # test\n",
    "    # batch_xt -> 테스트데이터\n",
    "    # batch_ys -> 테스트데이터 라벨\n",
    "    sess.run(train, feed_dict = { x: batch_xs, y_onehot : batch_ys })\n",
    "    if i % 600 == 0:\n",
    "        print(int(i/600), '에폭 훈련 정확도', sess.run(accuracy, feed_dict = { x: batch_xs, y_onehot : batch_ys })) # 정확도 출력을 위해 accuracy\n",
    "        print(int(i/600), '에폭 테스트 정확도', sess.run(accuracy, feed_dict = { x: batch_xt, y_onehot : batch_yt })) # 정확도 출력을 위해 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ CNN 계층 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.reset_default_graph()\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "#입력층\n",
    "x = tf.placeholder(\"float\",[None,784]) # None은 배치를 처리하기 위해서 작성\n",
    "x1 = tf.reshape(x,[-1,28,28,1]) # 2차원을 4차원으로 변경한다.\n",
    "\n",
    "\n",
    "#은닉1층(CNN층)\n",
    "b1 = tf.Variable(tf.ones([32])) # 숫자 1로 채워진 바이어스를 생성\n",
    "W1 = tf.Variable(tf.random_normal([5,5,1,32], stddev = 0.01)) # \n",
    "y1 = tf.nn.conv2d(x1, W1, strides=[1,1,1,1], padding = 'SAME') \n",
    "# convolution을 tf.nn.conv2d로 나타냄.\n",
    "# stride가 2라면 = [1,2,2,1]\n",
    "y1 = y1 + b1\n",
    "y1 = tf.nn.relu(y1)\n",
    "y1 = tf.nn.max_pool(y1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME') \n",
    "# ksize : kernal(filter) 사이즈\n",
    "\n",
    "#은닉2층(CNN층)\n",
    "b2 = tf.Variable(tf.ones([64])) # 숫자 1로 채워진 바이어스를 생성\n",
    "W2 = tf.Variable(tf.random_normal([5,5,32,64],stddev = 0.01))\n",
    "y2 = tf.nn.conv2d(y1, W2, strides=[1,1,1,1], padding = 'SAME')\n",
    "y2 = y2 + b2\n",
    "y2 = tf.nn.relu(y2)\n",
    "\n",
    "y2 = tf.nn.max_pool(y2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "#은닉3층(fully connected 층)\n",
    "b3 = tf.Variable(tf.ones([100]))\n",
    "W3 = tf.get_variable(name='W3', shape=[7*7*64, 100], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "# W3 는 he로 가중치 초기화를 하는 상황\n",
    "\n",
    "y3 = tf.reshape(y2, [-1, 7*7*64])\n",
    "y3 = tf.matmul(y3,W3) + b3\n",
    "y3 = tf.contrib.layers.batch_norm(y3,True)\n",
    "\n",
    "#드롭아웃( random 으로 드롭아웃한다.)\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "y3_drop = tf.nn.dropout(y3, keep_prob)\n",
    "\n",
    "#출력층(4층)\n",
    "b4 = tf.Variable(tf.ones([10]))\n",
    "W4 = tf.get_variable(name='W4', shape=[100, 10], initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값\n",
    "y4 = tf.matmul(y3_drop,W4) + b4\n",
    "y4 = tf.contrib.layers.batch_norm(y4,True)\n",
    "y_hat = tf.nn.softmax(y4)\n",
    "\n",
    "\n",
    "#예측값\n",
    "y_predict = tf.argmax(y_hat,1)\n",
    "\n",
    "\n",
    "# 라벨을 저장하기 위한 변수 생성\n",
    "y_onehot = tf.placeholder(\"float\",[None,10])\n",
    "y_label = tf.argmax(y_onehot, axis = 1)\n",
    "\n",
    "\n",
    "# 정확도를 출력하기 위한 변수 생성\n",
    "correct_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "\n",
    "\n",
    "# 교차 엔트로피 오차 함수\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(y_hat), axis = 1)\n",
    "\n",
    "\n",
    "# SGD 경사 감소법\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "\n",
    "\n",
    "# Adam 경사 감소법\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "\n",
    "# 학습 오퍼레이션 정의\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for j in range(20):\n",
    "        for i in range(600):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            test_xs, test_ys = mnist.test.next_batch(100)\n",
    "\n",
    "            sess.run(train, feed_dict={x: batch_xs, y_onehot: batch_ys, keep_prob: 0.8})\n",
    "\n",
    "            if i == 0:\n",
    "                train_acc = sess.run(accuracy, feed_dict={x: batch_xs, y_onehot: batch_ys, keep_prob: 1.0})\n",
    "                test_acc = sess.run(accuracy, feed_dict={x: test_xs, y_onehot: test_ys, keep_prob: 1.0})\n",
    "\n",
    "                train_acc_list.append(train_acc)\n",
    "                test_acc_list.append(test_acc)\n",
    "\n",
    "                print('훈련', str(j + 1) + '에폭 정확도 :', train_acc)\n",
    "                print('테스트', str(j + 1) + '에폭 정확도 :', test_acc)\n",
    "                print('-----------------------------------------------')\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot()\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(min(min(train_acc_list),min(test_acc_list))-0.1, 1.1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 문제102. 위의 신경망을 아래와 같이 수정하시오\n",
    "\n",
    "기존층 : conv1 --> pooling --> conv2 --> pooling --> fc1 --> fc2(출력층)  \n",
    "변경된층 : conv1 --> pooling --> fc1 --> fc2(출력층)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 이란 ?\n",
    "\n",
    "cifar10은 총 60000개 데이터 셋으로 이루어져 있으며 그 중 50000개는 훈련 데이터이고 10000개가 테스트 데이터이다.  \n",
    "class는 비행기부터 트럭까지 총 10개로 구성되어있다.   \n",
    "\n",
    "\n",
    "1. 비행기  \n",
    "2. 자동차  \n",
    "3. 새  \n",
    "4. 고양이  \n",
    "5. 사슴  \n",
    "6. 개  \n",
    "7. 개구리  \n",
    "8. 말  \n",
    "9. 배  \n",
    "10. 트럭  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ cifar10 데이터를 신경망에 로드하는 파이썬 코드 작성\n",
    "\n",
    "D:\\data\\cifar10\\train100 폴더를 하나 만들고 훈련데이터 100장(1~100번)을 복사해서 붙여넣으시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제103. D:\\\\data\\\\cifar10\\\\train100 에서 사진을 불러와서 아래와 같이 이미지 이름을 출력하는 함수를 생성하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_image = D:\\data\\cifar10\\train100  \n",
    "print(image_load(train_image))  \n",
    "\n",
    "['1.png', '10.png', '100.png'......., '99.png']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.png', '10.png', '100.png', '11.png', '12.png', '13.png', '14.png', '15.png', '16.png', '17.png', '18.png', '19.png', '2.png', '20.png', '21.png', '22.png', '23.png', '24.png', '25.png', '26.png', '27.png', '28.png', '29.png', '3.png', '30.png', '31.png', '32.png', '33.png', '34.png', '35.png', '36.png', '37.png', '38.png', '39.png', '4.png', '40.png', '41.png', '42.png', '43.png', '44.png', '45.png', '46.png', '47.png', '48.png', '49.png', '5.png', '50.png', '51.png', '52.png', '53.png', '54.png', '55.png', '56.png', '57.png', '58.png', '59.png', '6.png', '60.png', '61.png', '62.png', '63.png', '64.png', '65.png', '66.png', '67.png', '68.png', '69.png', '7.png', '70.png', '71.png', '72.png', '73.png', '74.png', '75.png', '76.png', '77.png', '78.png', '79.png', '8.png', '80.png', '81.png', '82.png', '83.png', '84.png', '85.png', '86.png', '87.png', '88.png', '89.png', '9.png', '90.png', '91.png', '92.png', '93.png', '94.png', '95.png', '96.png', '97.png', '98.png', '99.png']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train100'\n",
    "    \n",
    "def image_load(path):\n",
    "    file_list = os.listdir(path)\n",
    "    return file_list\n",
    "\n",
    "print(image_load(train_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제104. 위의 결과에서 .png는 빼고 숫자만 출력되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10, 100, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 4, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 5, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 6, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 7, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 8, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 9, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train100'\n",
    "    \n",
    "def image_load(path):\n",
    "    file_list = os.listdir(path)\n",
    "    file_name = []\n",
    "    for i in file_list:\n",
    "        a = int(re.sub('[^0-9]', '', i)) #숫자가 아닌것은 ''으로 처리\n",
    "        file_name.append(a)\n",
    "    return file_name\n",
    "\n",
    "print(image_load(train_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제105. 위의 결과를 정렬되어서 출력되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train100'\n",
    "    \n",
    "def image_load(path):\n",
    "    file_list = os.listdir(path)\n",
    "    file_name = []\n",
    "    for i in file_list:\n",
    "        a = int(re.sub('[^0-9]', '', i)) #숫자가 아닌것은 ''으로 처리\n",
    "        file_name.append(a)\n",
    "    file_name.sort()\n",
    "    return file_name\n",
    "\n",
    "print(image_load(train_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제106. 위의 출력된 결과에 다시 .png를 붙여서 아래와 같이 출력되게 하시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.png', '2.png', '3.png', '4.png', '5.png', '6.png', '7.png', '8.png', '9.png', '10.png', '11.png', '12.png', '13.png', '14.png', '15.png', '16.png', '17.png', '18.png', '19.png', '20.png', '21.png', '22.png', '23.png', '24.png', '25.png', '26.png', '27.png', '28.png', '29.png', '30.png', '31.png', '32.png', '33.png', '34.png', '35.png', '36.png', '37.png', '38.png', '39.png', '40.png', '41.png', '42.png', '43.png', '44.png', '45.png', '46.png', '47.png', '48.png', '49.png', '50.png', '51.png', '52.png', '53.png', '54.png', '55.png', '56.png', '57.png', '58.png', '59.png', '60.png', '61.png', '62.png', '63.png', '64.png', '65.png', '66.png', '67.png', '68.png', '69.png', '70.png', '71.png', '72.png', '73.png', '74.png', '75.png', '76.png', '77.png', '78.png', '79.png', '80.png', '81.png', '82.png', '83.png', '84.png', '85.png', '86.png', '87.png', '88.png', '89.png', '90.png', '91.png', '92.png', '93.png', '94.png', '95.png', '96.png', '97.png', '98.png', '99.png', '100.png']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "train_image = 'D:\\\\data\\\\cifar10\\\\train100'\n",
    "    \n",
    "def image_load(path):\n",
    "    file_list = os.listdir(path)\n",
    "    file_name = []\n",
    "    for i in file_list:\n",
    "        a = int(re.sub('[^0-9]', '', i)) #숫자가 아닌것은 ''으로 처리\n",
    "        file_name.append(a)\n",
    "        file_name.sort()\n",
    "    file_res = []\n",
    "    for j in file_name:\n",
    "        file_res.append('%d.png' %j)\n",
    "    \n",
    "    return file_res\n",
    "\n",
    "print(image_load(train_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제109. 훈련이미지의 라벨을 one hot encoding 하는 코드를 구현하시오!\n",
    "\n",
    "train_label.csv 파일의 위치를 지정\n",
    "\n",
    "train_label = 'D:\\\\data\\\\cifar10\\\\train_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def label_load(path):\n",
    "    file = open(path)\n",
    "    labeldata = csv.reader(file)\n",
    "    labellist = []\n",
    "    for i in labeldata:\n",
    "        labellist.append(i)\n",
    "    return labelist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
