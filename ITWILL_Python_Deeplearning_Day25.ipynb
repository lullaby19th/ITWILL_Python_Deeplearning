{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")\n",
    "\n",
    "train_x = mnist.train.images\n",
    "train_y = mnist.train.labels\n",
    "\n",
    "total_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "\n",
    "def generator( z , reuse = False ) :\n",
    "    if reuse==False :\n",
    "        with tf.variable_scope(name_or_scope = \"Gen\") as scope :\n",
    "            gw1 = tf.get_variable(name = \"w1\",\n",
    "                                  shape = [128, 256],\n",
    "                                  initializer= tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
    "            # quiz : weight 의 초깃값을 random normal 로 주는 이유는 무엇일까요?\n",
    "\n",
    "            gb1 = tf.get_variable(name = \"b1\",\n",
    "                                 shape = [256],\n",
    "                                 initializer = tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
    "\n",
    "            gw2 = tf.get_variable(name = \"w2\",\n",
    "                                  shape = [256, 784],\n",
    "                                  initializer= tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
    "\n",
    "            gb2 = tf.get_variable(name = \"b2\",\n",
    "                                 shape = [784],\n",
    "                                 initializer = tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
    "    else :\n",
    "        with tf.variable_scope(name_or_scope=\"Gen\", reuse = True) as scope :\n",
    "            gw1 = tf.get_variable(name = \"w1\",\n",
    "                                  shape = [128, 256],\n",
    "                                  initializer= tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
    "\n",
    "            gb1 = tf.get_variable(name = \"b1\",\n",
    "                                 shape = [256],\n",
    "                                 initializer = tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
    "\n",
    "            gw2 = tf.get_variable(name = \"w2\",\n",
    "                                  shape = [256, 784],\n",
    "                                  initializer= tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
    "\n",
    "            gb2 = tf.get_variable(name = \"b2\",\n",
    "                                 shape = [784],\n",
    "                                 initializer = tf.random_normal_initializer(mean=0.0, stddev = 0.01))\n",
    "\n",
    "\n",
    "    hidden = tf.nn.relu( tf.matmul(z , gw1) + gb1 )\n",
    "    output = tf.nn.sigmoid( tf.matmul(hidden, gw2) + gb2 )   #출력층에 시그모이드를 쓰는 이유는 무엇일까요?\n",
    "\n",
    "    return output   #[784] 가짜 생성된 이미지\n",
    "\n",
    "\n",
    "\n",
    "def discriminator( x , reuse = False) :\n",
    "\n",
    "    if(reuse == False) :\n",
    "        with tf.variable_scope(name_or_scope=\"Dis\") as scope :\n",
    "            dw1 = tf.get_variable(name = \"w1\",\n",
    "                                  shape = [784, 256],\n",
    "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
    "            db1 = tf.get_variable(name = \"b1\",\n",
    "                                  shape = [256],\n",
    "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
    "            dw2 = tf.get_variable(name = \"w2\",\n",
    "                                  shape = [256, 1],\n",
    "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
    "            db2 = tf.get_variable(name = \"b2\",\n",
    "                                  shape = [1],\n",
    "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
    "    else :\n",
    "        with tf.variable_scope(name_or_scope=\"Dis\", reuse = True) as scope :\n",
    "            dw1 = tf.get_variable(name = \"w1\",\n",
    "                                  shape = [784, 256],\n",
    "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
    "            db1 = tf.get_variable(name = \"b1\",\n",
    "                                  shape = [256],\n",
    "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
    "            dw2 = tf.get_variable(name = \"w2\",\n",
    "                                  shape = [256, 1],\n",
    "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
    "            db2 = tf.get_variable(name = \"b2\",\n",
    "                                  shape = [1],\n",
    "                                  initializer = tf.random_normal_initializer(0.0, 0.01) )\n",
    "\n",
    "    hidden = tf.nn.relu( tf.matmul(x , dw1) + db1 )  #[-, 256]\n",
    "    output = tf.nn.sigmoid( tf.matmul(hidden, dw2)  + db2 )   #[-, 1]  진품인지(1) 가품인지(0)의 label 결과값\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def random_noise(batch_size) :\n",
    "    return np.random.normal(size=[batch_size , 128])\n",
    "\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default() :\n",
    "\n",
    "    ######################################################\n",
    "    # 1 .Feedable part  :: 그래프에서 유일하게 데이터가 유입될 수 있는 장소\n",
    "    ######################################################\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, 784]) #GAN 은 autoencoder 와 마찬가지로 unsupervised learning 이므로 y(label)을 사용하지 않습니다.\n",
    "\n",
    "    Z = tf.placeholder(tf.float32, [None, 128]) #Z 는 생성기의 입력값으로 사용될 노이즈 입니다.\n",
    "\n",
    "\n",
    "    ################################\n",
    "    # 2. generator 와 discriminator 의 사용\n",
    "    ##################################\n",
    "\n",
    "\n",
    "    fake_x = generator(Z)\n",
    "\n",
    "    result_of_fake = discriminator(fake_x)\n",
    "    result_of_real = discriminator(X , True)\n",
    "\n",
    "\n",
    "    ################################\n",
    "    # 3. Loss( 성취도평가 ) : g_loss 와 d_loss\n",
    "\n",
    "    # g_loss & d_loss 모두 높을 수록 좋다.\n",
    "    # g_loss : 얼마나 fake_x 가 진짜같은가\n",
    "    # d_loss : 얼마나 discriminator 가 정확한가\n",
    "\n",
    "    # 두 수치를 모두 높이도록 train 하면 생성기와 분류기의 성능이 모두 올라간다.\n",
    "    ################################\n",
    "\n",
    "    g_loss = tf.reduce_mean( tf.log(result_of_fake) )\n",
    "    d_loss = tf.reduce_mean( tf.log(result_of_real) + tf.log(1 - result_of_fake) )\n",
    "\n",
    "\n",
    "    ################################\n",
    "    # 4. Train : Maximizing g_loss & d_loss\n",
    "    ################################\n",
    "\n",
    "    t_vars = tf.trainable_variables() # return list\n",
    "\n",
    "    g_vars = [var for var in t_vars if \"Gen\" in var.name]\n",
    "    d_vars = [var for var in t_vars if \"Dis\" in var.name]\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    g_train = optimizer.minimize(-g_loss, var_list= g_vars)\n",
    "    d_train = optimizer.minimize(-d_loss, var_list = d_vars)    \n",
    "\n",
    "    # g_loss & d_loss 를 최대화 시켜야하는데 minimize 함수밖에 없기 때문에 - 음수부호 붙인다.\n",
    "    \n",
    "with tf.Session(graph = g) as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    total_batchs = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "    for epoch in range(total_epochs) :\n",
    "\n",
    "        for batch in range(total_batchs) :\n",
    "            batch_x = train_x[batch * batch_size : (batch+1) * batch_size]  # [batch_size , 784]\n",
    "            batch_y = train_y[batch * batch_size : (batch+1) * batch_size]  # [batch_size,]\n",
    "            noise = random_noise(batch_size)  # [batch_size, 128]\n",
    "\n",
    "            sess.run(g_train , feed_dict = {Z : noise})\n",
    "            sess.run(d_train, feed_dict = {X : batch_x , Z : noise})\n",
    "\n",
    "            gl, dl = sess.run([g_loss, d_loss], feed_dict = {X : batch_x , Z : noise})\n",
    "\n",
    "\n",
    "        #매 20 epoch 마다 학습된 성능을 중간점검  (실제론 더 자주하시는 것이 좋습니다. )\n",
    "        if (epoch+1) % 20 == 0 or epoch == 1  :\n",
    "            print(\"=======Epoch : \", epoch , \" =======================================\")\n",
    "            print(\"생성기 성능 : \" ,gl )\n",
    "            print(\"분류기 성능 : \" ,dl )\n",
    "            print(\"생성기와 분류기 선의의 경쟁중...\")\n",
    "\n",
    "\n",
    "        #10개의 epoch 마다 Generator 가 만들어내는 실제 결과물을 얻어보며 , 성능 발전을 시각적으로 확인\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % 10 == 0  :\n",
    "            sample_noise = random_noise(10)\n",
    "\n",
    "            generated = sess.run(fake_x , feed_dict = { Z : sample_noise})\n",
    "\n",
    "            fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
    "            for i in range(10) :\n",
    "                ax[i].set_axis_off()\n",
    "                ax[i].imshow( np.reshape( generated[i], (28, 28)) )\n",
    "\n",
    "            plt.savefig('D:\\\\gandata\\\\{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "\n",
    "    print('최적화 완료!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
